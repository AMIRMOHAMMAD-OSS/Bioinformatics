{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AMIRMOHAMMAD-OSS/Bioinformatics/blob/main/Final_version_fornow.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NdTCu1zGi60i",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title Importing libraries\n",
        "%%capture\n",
        "from google.colab import output\n",
        "!git clone https://github.com/AMIRMOHAMMAD-OSS/Phaseek\n",
        "!pip install biopython\n",
        "!apt-get update\n",
        "!apt-get install -y ncbi-blast+\n",
        "!pip install streamlit\n",
        "\n",
        "!pip install py3Dmol\n",
        "!pip install requests\n",
        "!pip install biotite\n",
        "!pip install stmol\n",
        "!pip install command_runner\n",
        "from command_runner import command_runner\n",
        "!pip install plotly\n",
        "!pip install --upgrade \"kaleido==0.1.*\"\n",
        "import kaleido\n",
        "from Bio import SeqIO\n",
        "from Bio.Seq import Seq\n",
        "from Bio import pairwise2\n",
        "from google.colab import files\n",
        "import plotly.graph_objects as go\n",
        "from plotly.subplots import make_subplots\n",
        "import zipfile\n",
        "import shutil\n",
        "from transformers import AutoTokenizer\n",
        "from tqdm import tqdm\n",
        "import math\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "import numpy as np\n",
        "import time\n",
        "from collections import defaultdict\n",
        "import torch\n",
        "from torch.utils.data.dataloader import DataLoader\n",
        "import time\n",
        "import os\n",
        "import sys\n",
        "import json\n",
        "import random\n",
        "from ast import literal_eval\n",
        "import numpy as np\n",
        "import torch\n",
        "from transformers import AutoTokenizer\n",
        "from transformers import AutoTokenizer\n",
        "from tokenizers import Tokenizer, decoders, models, normalizers, pre_tokenizers, trainers\n",
        "from transformers import CanineTokenizer, CanineModel"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aznM7fXF5MnI",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title Importing libraries\n",
        "%%capture\n",
        "\n",
        "tokenizer = Tokenizer.from_file(\"/content/Phaseek/model/Trained_BPE2.json\")\n",
        "tokenizer.model_max_length = 256\n",
        "tokenizer\n",
        "\n",
        "from sklearn.preprocessing import StandardScaler as Sc\n",
        "from sklearn.decomposition import PCA\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "import torch\n",
        "\n",
        "from sklearn.model_selection import train_test_split as tts\n",
        "\n",
        "\n",
        "def edit(x):\n",
        "    for i in x:\n",
        "      if i in \"BJOUX\\Z_n\\n\":\n",
        "        x = x.replace(i,\"\")\n",
        "      else:\n",
        "        pass\n",
        "    return x\n",
        "\n",
        "def encode(x):\n",
        "  l = []\n",
        "  chars = tokenizer.get_vocab()\n",
        "  for i in x:\n",
        "    l.append(chars[i])\n",
        "  return l\n",
        "import pickle\n",
        "\n",
        "with open('/content/Phaseek/model/xgb_model.pkl', 'rb') as f:\n",
        "    clf = pickle.load(f)\n",
        "\n",
        "\n",
        "chars = tokenizer.get_vocab()\n",
        "vocab_size = len(tokenizer.get_vocab())\n",
        "\n",
        "def Padding(x,PAD = 0,max_len = 512):\n",
        "  l = []\n",
        "  max_len = max(512,len(max(x,key = lambda x: len(x))))\n",
        "  for i in x:\n",
        "    a = i\n",
        "    if len(a) < max_len:\n",
        "      a = a + [PAD for i in range(max_len-len(a))]\n",
        "    l.append(a)\n",
        "  return np.array(l).reshape(len(x),max_len)\n",
        "\n",
        "print(\"Everything looks ok! âœ¨ðŸ°âœ¨\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DnIpemCr5Rxp",
        "outputId": "efb56e54-5641-4cdb-b8ac-a982abd89649"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cuda is not available ðŸ˜”\n",
            "\n",
            "The allocated device is : cpu\n"
          ]
        }
      ],
      "source": [
        "#@title Loading models and requierments\n",
        "if torch.cuda.is_available():\n",
        "  print(\"Cuda is available ðŸŽ‰\\n\")\n",
        "  print(\"The allocated device is : \"+torch.cuda.get_device_name())\n",
        "  device = torch.device(\"cuda\")\n",
        "else:\n",
        "  print(\"Cuda is not available ðŸ˜”\\n\")\n",
        "  print(\"The allocated device is : cpu\")\n",
        "  device = torch.device(\"cpu\")\n",
        "\n",
        "\n",
        "\n",
        "def set_seed(seed):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "def setup_logging(config):\n",
        "    work_dir = config.system.work_dir\n",
        "    os.makedirs(work_dir, exist_ok=True)\n",
        "    with open(os.path.join(work_dir, 'args.txt'), 'w') as f:\n",
        "        f.write(' '.join(sys.argv))\n",
        "    with open(os.path.join(work_dir, 'config.json'), 'w') as f:\n",
        "        f.write(json.dumps(config.to_dict(), indent=4))\n",
        "\n",
        "class CfgNode:\n",
        "    def __init__(self, **kwargs):\n",
        "        self.__dict__.update(kwargs)\n",
        "\n",
        "    def __str__(self):\n",
        "        return self._str_helper(0)\n",
        "\n",
        "    def _str_helper(self, indent):\n",
        "        parts = []\n",
        "        for k, v in self.__dict__.items():\n",
        "            if isinstance(v, CfgNode):\n",
        "                parts.append(\"%s:\\n\" % k)\n",
        "                parts.append(v._str_helper(indent + 1))\n",
        "            else:\n",
        "                parts.append(\"%s: %s\\n\" % (k, v))\n",
        "        parts = [' ' * (indent * 4) + p for p in parts]\n",
        "        return \"\".join(parts)\n",
        "\n",
        "    def to_dict(self):\n",
        "        return { k: v.to_dict() if isinstance(v, CfgNode) else v for k, v in self.__dict__.items() }\n",
        "\n",
        "    def merge_from_dict(self, d):\n",
        "        self.__dict__.update(d)\n",
        "\n",
        "    def merge_from_args(self, args):\n",
        "\n",
        "        for arg in args:\n",
        "            keyval = arg.split('=')\n",
        "            assert len(keyval) == 2, \"expecting each override arg to be of form --arg=value, got %s\" % arg\n",
        "            key, val = keyval\n",
        "            try:\n",
        "                val = literal_eval(val)\n",
        "            except ValueError:\n",
        "                pass\n",
        "\n",
        "            assert key[:2] == '--'\n",
        "            key = key[2:]\n",
        "            keys = key.split('.')\n",
        "            obj = self\n",
        "            for k in keys[:-1]:\n",
        "                obj = getattr(obj, k)\n",
        "            leaf_key = keys[-1]\n",
        "            assert hasattr(obj, leaf_key), f\"{key} is not an attribute that exists in the config\"\n",
        "            print(\"command line overwriting config attribute %s with %s\" % (key, val))\n",
        "            setattr(obj, leaf_key, val)\n",
        "\n",
        "import math\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "\n",
        "class SiLU(nn.Module):\n",
        "   def forward(self, x):\n",
        "        return x*F.sigmoid(x)\n",
        "\n",
        "class NY(nn.Module):\n",
        "  def forward(self,x):\n",
        "    return 3*torch.tanh(0.3*x)\n",
        "\n",
        "class NewGELU(nn.Module):\n",
        "    def forward(self, x):\n",
        "        return 0.5 * x * (1.0 + torch.tanh(math.sqrt(2.0 / math.pi) * (x + 0.044715 * torch.pow(x, 3.0))))\n",
        "\n",
        "class SelfAttention(nn.Module):\n",
        "\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        assert config.n_embd % config.n_head == 0\n",
        "        self.c_attn = nn.Linear(config.n_embd, 3 * config.n_embd)\n",
        "        self.c_proj = nn.Linear(config.n_embd, config.n_embd)\n",
        "        self.attn_dropout = nn.Dropout(config.attn_pdrop)\n",
        "        self.resid_dropout = nn.Dropout(config.resid_pdrop)\n",
        "        self.register_buffer(\"bias\", torch.tril(torch.ones(config.block_size, config.block_size))\n",
        "                                     .view(1, 1, config.block_size, config.block_size))\n",
        "        self.n_head = config.n_head\n",
        "        self.n_embd = config.n_embd\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, T, C = x.size()\n",
        "        q, k ,v  = self.c_attn(x).split(self.n_embd, dim=2)\n",
        "        k = k.view(B, T, self.n_head, C // self.n_head).transpose(1, 2)\n",
        "        q = q.view(B, T, self.n_head, C // self.n_head).transpose(1, 2)\n",
        "        v = v.view(B, T, self.n_head, C // self.n_head).transpose(1, 2)\n",
        "        att = (q @ k.transpose(-2, -1)) * (1.0 / math.sqrt(k.size(-1)))\n",
        "        att = att.masked_fill(self.bias[:,:,:T,:T] == 0, float('-inf'))\n",
        "        att = F.softmax(att, dim=-1)\n",
        "        att = self.attn_dropout(att)\n",
        "        y = att @ v\n",
        "        y = y.transpose(1, 2).contiguous().view(B, T, C)\n",
        "        y = self.resid_dropout(self.c_proj(y))\n",
        "        return y\n",
        "\n",
        "class Block(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.ln_1 = nn.LayerNorm(config.n_embd)\n",
        "        self.attn = SelfAttention(config)\n",
        "        self.ln_2 = nn.LayerNorm(config.n_embd)\n",
        "        self.mlp = nn.ModuleDict(dict(\n",
        "            c_fc    = nn.Linear(config.n_embd, 4 * config.n_embd),\n",
        "            c_proj  = nn.Linear(4 * config.n_embd, config.n_embd),\n",
        "            act     = NewGELU(),\n",
        "            dropout = nn.Dropout(config.resid_pdrop),\n",
        "        ))\n",
        "        m = self.mlp\n",
        "        self.mlpf = lambda x: m.dropout(m.c_proj(m.act(m.c_fc(x))))\n",
        "    def forward(self, x):\n",
        "        x = x + self.attn(self.ln_1(x))\n",
        "        x = x + self.mlpf(self.ln_2(x))\n",
        "        return x\n",
        "\n",
        "class ClassifierII(nn.Module):\n",
        "    @staticmethod\n",
        "    def get_default_config():\n",
        "        C = CfgNode()\n",
        "        C.model_type = 'gpt'\n",
        "        C.n_layer = None\n",
        "        C.n_head = None\n",
        "        C.n_embd =  None\n",
        "        C.vocab_size = len(chars)\n",
        "        C.max_length = 512\n",
        "        C.embd_pdrop = 0.1\n",
        "        C.resid_pdrop = 0.1\n",
        "        C.attn_pdrop = 0.1\n",
        "        return C\n",
        "\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.max_length = config.block_size\n",
        "        self.soft = nn.Softmax(1)\n",
        "        self.config = self.get_default_config()\n",
        "        self.device = \"cuda\"\n",
        "        self.model_states = {'h':{'n_layer': 48, 'n_head': 25, 'n_embd': 1600},\n",
        "                                    'g':{'n_layer': 12, '': 12, 'n_embd': 768},\n",
        "                'f':   {'n_layer': 24, 'n_head': 16, 'n_embd': 1024},\n",
        "                'e':   {'n_layer': 36, 'n_head': 20, 'n_embd': 1280},\n",
        "                'd':{'n_layer': 8, 'n_head': 16, 'n_embd': 512},\n",
        "                'c':{'n_layer': 6, 'n_head': 6, 'n_embd': 192},\n",
        "                'b':{'n_layer': 4, 'n_head': 4, 'n_embd': 128},\n",
        "                'a':{'n_layer': 3, 'n_head': 3, 'n_embd': 48}}\n",
        "\n",
        "        type_ = config.model_type is not None\n",
        "        #assert type_ in \"abcdefgh\"\n",
        "        p = all([config.n_layer is not None, config.n_head is not None, config.n_embd is not None])\n",
        "        #assert type_ == True and p == True\n",
        "        if type_:\n",
        "            config.merge_from_dict(self.model_states[config.model_type])\n",
        "        self.closs = nn.BCELoss()\n",
        "        self.ny = NY()\n",
        "        self.l = nn.Linear(512,1,64)\n",
        "        self.transformer = nn.ModuleDict(dict(\n",
        "            wte = nn.Embedding(config.vocab_size, config.n_embd),\n",
        "            wpe = nn.Embedding(config.max_length, config.n_embd),\n",
        "            drop = nn.Dropout(config.embd_pdrop),\n",
        "            h = nn.ModuleList([Block(config) for _ in range(config.n_layer)]),\n",
        "            ln_f = nn.LayerNorm(config.n_embd),))\n",
        "        self.classifier_head = nn.Sequential(#nn.Tanh(),\n",
        "                                             nn.Linear(config.n_embd, 2)\n",
        "                                             )\n",
        "        self.apply(self._init_weights)\n",
        "        for pn, p in self.named_parameters():\n",
        "            if pn.endswith('c_proj.weight'):\n",
        "                torch.nn.init.normal_(p, mean=0.0, std=0.02/math.sqrt(2 * config.n_layer))\n",
        "\n",
        "        n_params = sum(p.numel() for p in self.transformer.parameters())\n",
        "        print(\"[ Number of trainable parameters: %.2fM ]\" % (n_params/1e6,))\n",
        "\n",
        "    def _init_weights(self, module):\n",
        "        if isinstance(module, nn.Linear):\n",
        "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
        "            if module.bias is not None:\n",
        "                torch.nn.init.zeros_(module.bias)\n",
        "        elif isinstance(module, nn.Embedding):\n",
        "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
        "        elif isinstance(module, nn.LayerNorm):\n",
        "            torch.nn.init.zeros_(module.bias)\n",
        "            torch.nn.init.ones_(module.weight)\n",
        "\n",
        "    @classmethod\n",
        "    def from_pretrained(cls, model_type):\n",
        "        from transformers import GPT2LMHeadModel\n",
        "        config = cls.get_default_config()\n",
        "        config.model_type = model_type\n",
        "        config.vocab_size = 25\n",
        "        config.max_length = 512\n",
        "        model = ClassifierII(config)\n",
        "        sd = model.state_dict()\n",
        "        model_hf = GPT2LMHeadModel.from_pretrained(model_type)\n",
        "        sd_hf = model_hf.state_dict()\n",
        "        keys = [k for k in sd_hf if not k.endswith('attn.masked_bias')]\n",
        "        transposed = ['attn.c_attn.weight', 'attn.c_proj.weight', 'mlp.c_fc.weight', 'mlp.c_proj.weight']\n",
        "        assert len(keys) == len(sd)\n",
        "        for k in keys:\n",
        "            if any(k.endswith(w) for w in transposed):\n",
        "                assert sd_hf[k].shape[::-1] == sd[k].shape\n",
        "                with torch.no_grad():\n",
        "                    sd[k].copy_(sd_hf[k].t())\n",
        "            else:\n",
        "                assert sd_hf[k].shape == sd[k].shape\n",
        "                with torch.no_grad():\n",
        "                    sd[k].copy_(sd_hf[k])\n",
        "        return model\n",
        "\n",
        "    def configure_optimizers(self, train_config):\n",
        "        decay = set()\n",
        "        no_decay = set()\n",
        "        whitelist_weight_modules = (torch.nn.Linear, )\n",
        "        blacklist_weight_modules = (torch.nn.LayerNorm, torch.nn.Embedding)\n",
        "        for mn, m in self.named_modules():\n",
        "            for pn, p in m.named_parameters():\n",
        "                fpn = '%s.%s' % (mn, pn) if mn else pn\n",
        "                if pn.endswith('bias'):\n",
        "                    no_decay.add(fpn)\n",
        "                elif pn.endswith('weight') and isinstance(m, whitelist_weight_modules):\n",
        "                    decay.add(fpn)\n",
        "                elif pn.endswith('weight') and isinstance(m, blacklist_weight_modules):\n",
        "                    no_decay.add(fpn)\n",
        "        param_dict = {pn: p for pn, p in self.named_parameters()}\n",
        "        inter_params = decay & no_decay\n",
        "        union_params = decay | no_decay\n",
        "        assert len(inter_params) == 0, \"parameters %s made it into both decay/no_decay sets!\" % (str(inter_params), )\n",
        "        assert len(param_dict.keys() - union_params) == 0, \"parameters %s were not separated into either decay/no_decay set!\" \\\n",
        "                                                    % (str(param_dict.keys() - union_params), )\n",
        "        optim_groups = [\n",
        "            {\"params\": [param_dict[pn] for pn in sorted(list(decay))], \"weight_decay\": train_config.weight_decay},\n",
        "            {\"params\": [param_dict[pn] for pn in sorted(list(no_decay))], \"weight_decay\": 0.0},]\n",
        "        optimizer = torch.optim.AdamW(optim_groups, lr=train_config.learning_rate, betas=train_config.betas)\n",
        "        return optimizer\n",
        "\n",
        "    @torch.no_grad\n",
        "    def predict_proba(self,idx,j =\"None\"):\n",
        "      sigmoid = nn.Sigmoid()\n",
        "      soft = nn.Softmax(dim=1)\n",
        "      ny = NY()\n",
        "      si = SiLU()\n",
        "      self.eval()\n",
        "      x,_ = self.forward(idx)\n",
        "      if j == \"None\":\n",
        "        return x[:,0:1]\n",
        "      elif j == \"soft\":\n",
        "        return self.soft(x)[:,0:1]\n",
        "      else:\n",
        "        return sigmoid(x)[:,0:1]\n",
        "\n",
        "    @torch.no_grad\n",
        "    def predict(self,idx):\n",
        "      g = torch.zeros((idx.shape[0],1))\n",
        "      e = -1\n",
        "      for i in self.predict_proba(idx):\n",
        "        e+=1\n",
        "        if i[0].item()>=0.5:\n",
        "          g[e] = 1\n",
        "      return g\n",
        "\n",
        "    def forward(self, idx, targets=None):\n",
        "        device = idx.device\n",
        "        b, t = idx.size()\n",
        "        pos = torch.arange(0, t, dtype=torch.long, device=device).unsqueeze(0)\n",
        "        tok_emb = self.transformer.wte(idx)\n",
        "        pos_emb = self.transformer.wpe(pos)\n",
        "        x = self.transformer.drop(tok_emb + pos_emb)\n",
        "        for block in self.transformer.h:\n",
        "            x = block(x)\n",
        "        x = self.transformer.ln_f(x)\n",
        "        logits = self.classifier_head(x).view(b,2,t).to(device)\n",
        "        logits = self.l(logits)\n",
        "        logits = F.sigmoid(self.ny(logits).view(b,2).mean(1).view(b,1))\n",
        "        loss = None\n",
        "        if targets is not None:\n",
        "            loss = F.binary_cross_entropy(logits,targets)\n",
        "        return logits, loss\n",
        "\n",
        "\n",
        "class Trainer:\n",
        "    @staticmethod\n",
        "    def get_default_config():\n",
        "        C = CfgNode()\n",
        "        C.device = 'cuda'\n",
        "        C.num_workers = 4\n",
        "        C.max_iters = None\n",
        "        C.batch_size = 128\n",
        "        C.max_length = 512\n",
        "        C.learning_rate = 8e-4\n",
        "        C.betas = (0.9, 0.95)\n",
        "        C.weight_decay = 0.1\n",
        "        C.grad_norm_clip = 1.0\n",
        "        return C\n",
        "\n",
        "    def __init__(self, config, model, train_dataset,val_dataset):\n",
        "        self.config = config\n",
        "        self.model = model\n",
        "        self.optimizer = None\n",
        "        self.train_dataset = train_dataset\n",
        "        self.val_dataset = val_dataset\n",
        "        self.callbacks = defaultdict(list)\n",
        "        self.device ='cuda'\n",
        "        self.model = self.model.to(self.device)\n",
        "        self.iter_num = 0\n",
        "        self.iter_time = 0.0\n",
        "        self.iter_dt = 0.0\n",
        "\n",
        "    def add_callback(self, onevent: str, callback):\n",
        "        self.callbacks[onevent].append(callback)\n",
        "\n",
        "    def set_callback(self, onevent: str, callback):\n",
        "        self.callbacks[onevent] = [callback]\n",
        "\n",
        "    def trigger_callbacks(self, onevent: str):\n",
        "        for callback in self.callbacks.get(onevent, []):\n",
        "            callback(self)\n",
        "\n",
        "    def run(self):\n",
        "        model, config = self.model, self.config\n",
        "        self.optimizer = model.configure_optimizers(config)\n",
        "        batch_size = self.config.batch_size\n",
        "        def get_batch(mode):\n",
        "            batch_size = 64\n",
        "            if mode == \"train\":\n",
        "              data = train_dataset\n",
        "              pos_train = torch.tensor(np.array([data[i:i+1,:] for i in range(data.shape[0]) if data[i,-1].item() == 1]).reshape(len([data[i:i+1,:] for i in range(data.shape[0]) if data[i,-1].item() == 1]),513))\n",
        "              neg_train = torch.tensor(np.array([data[i:i+1,:] for i in range(data.shape[0]) if data[i,-1].item() == 0]).reshape(len([data[i:i+1,:] for i in range(data.shape[0]) if data[i,-1].item() == 0]),513))\n",
        "              N = np.random.randint(batch_size)\n",
        "              ix_pos = np.random.randint(pos_train.shape[0]-N)\n",
        "              ix_neg = np.random.randint(neg_train.shape[0]-batch_size-N)\n",
        "              pos_data = pos_train[ix_pos:ix_pos+N,:]\n",
        "              neg_data = neg_train[ix_neg:ix_neg+batch_size-N,:]\n",
        "              data = torch.concat((pos_data,neg_data))\n",
        "              seq = data[:,:-1]\n",
        "              targets = torch.zeros((batch_size,1), device=\"cuda\")\n",
        "              o = -1\n",
        "              for i in data:\n",
        "                o+=1\n",
        "                if i[-1].item() == 1:\n",
        "                  targets[o] = 1\n",
        "                #else:\n",
        "                  #targets[o][1] = 1\n",
        "            else:\n",
        "              data = val_dataset\n",
        "              ix = np.random.randint(data.shape[0]-batch_size)\n",
        "              data = data[ix:ix+batch_size,:]\n",
        "              seq = data[:,:-1]\n",
        "              targets = torch.zeros((batch_size,1), device=\"cuda\")\n",
        "              o = -1\n",
        "              for i in data:\n",
        "                o+=1\n",
        "                if i[-1].item() == 1:\n",
        "                  targets[o] = 1\n",
        "                #else:\n",
        "                 # targets[o][1] = 1\n",
        "            targets = targets.view(batch_size,1).to(\"cuda\")\n",
        "            seq = seq.view(batch_size,512).to(\"cuda\")\n",
        "            return seq, targets\n",
        "\n",
        "        @torch.no_grad\n",
        "        def cross_val():\n",
        "          model.eval()\n",
        "          out = []\n",
        "          for i in [\"train\",\"val\"]:\n",
        "            losses = torch.zeros(200+1)\n",
        "            for k in range(200):\n",
        "              X,Y = get_batch(i)\n",
        "              logits,loss = model(X,Y)\n",
        "              losses[k]=loss.item()\n",
        "              out1 = losses.mean()\n",
        "            out.append(out1)\n",
        "          model.train()\n",
        "          return out\n",
        "        losses = cross_val()\n",
        "        LOSS = [losses]\n",
        "        print(\"\\n[train loss = {k}, val loss =  {j}]\\n\".format(k = losses[0],j = losses[1]))\n",
        "        model.train()\n",
        "        for epoch in range(20):\n",
        "          print(\"[epoch {o}] \\n\".format(o = epoch))\n",
        "          iters = 200\n",
        "          for i in range(iters):\n",
        "            if i == 0:\n",
        "              Y = \"| =\"\n",
        "            elif i == iters -1 :\n",
        "              Y = \"=> 100% |\"\n",
        "            else:\n",
        "              if i%(int(iters/50)) == 0 :\n",
        "                Y = \"=\"\n",
        "              else:\n",
        "                Y = \"\"\n",
        "            print(\"{y}\".format(y = Y),end=\"\")\n",
        "            x, y = get_batch(\"train\")\n",
        "            logits, self.loss = model(x, y)\n",
        "            model.zero_grad(set_to_none=True)\n",
        "            self.loss.backward()\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), config.grad_norm_clip)\n",
        "            self.optimizer.step()\n",
        "          losses = cross_val()\n",
        "          LOSS.append(losses)\n",
        "          print(LOSS)\n",
        "          print(\"\\n[train loss = {k}, val loss =  {j}]\\n\".format(k = losses[0],j = losses[1]))\n",
        "        PATH = \"model {h}\".format(h = model_config.model_type)\n",
        "        torch.save(model.state_dict(), PATH)\n",
        "\n",
        "def set_seed(seed):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "def setup_logging(config):\n",
        "    work_dir = config.system.work_dir\n",
        "    os.makedirs(work_dir, exist_ok=True)\n",
        "    with open(os.path.join(work_dir, 'args.txt'), 'w') as f:\n",
        "        f.write(' '.join(sys.argv))\n",
        "    with open(os.path.join(work_dir, 'config.json'), 'w') as f:\n",
        "        f.write(json.dumps(config.to_dict(), indent=4))\n",
        "\n",
        "class CfgNode:\n",
        "    def __init__(self, **kwargs):\n",
        "        self.__dict__.update(kwargs)\n",
        "\n",
        "    def __str__(self):\n",
        "        return self._str_helper(0)\n",
        "\n",
        "    def _str_helper(self, indent):\n",
        "        parts = []\n",
        "        for k, v in self.__dict__.items():\n",
        "            if isinstance(v, CfgNode):\n",
        "                parts.append(\"%s:\\n\" % k)\n",
        "                parts.append(v._str_helper(indent + 1))\n",
        "            else:\n",
        "                parts.append(\"%s: %s\\n\" % (k, v))\n",
        "        parts = [' ' * (indent * 4) + p for p in parts]\n",
        "        return \"\".join(parts)\n",
        "\n",
        "    def to_dict(self):\n",
        "        return { k: v.to_dict() if isinstance(v, CfgNode) else v for k, v in self.__dict__.items() }\n",
        "\n",
        "    def merge_from_dict(self, d):\n",
        "        self.__dict__.update(d)\n",
        "\n",
        "    def merge_from_args(self, args):\n",
        "\n",
        "        for arg in args:\n",
        "            keyval = arg.split('=')\n",
        "            assert len(keyval) == 2, \"expecting each override arg to be of form --arg=value, got %s\" % arg\n",
        "            key, val = keyval\n",
        "            try:\n",
        "                val = literal_eval(val)\n",
        "            except ValueError:\n",
        "                pass\n",
        "\n",
        "            assert key[:2] == '--'\n",
        "            key = key[2:]\n",
        "            keys = key.split('.')\n",
        "            obj = self\n",
        "            for k in keys[:-1]:\n",
        "                obj = getattr(obj, k)\n",
        "            leaf_key = keys[-1]\n",
        "            assert hasattr(obj, leaf_key), f\"{key} is not an attribute that exists in the config\"\n",
        "            print(\"command line overwriting config attribute %s with %s\" % (key, val))\n",
        "            setattr(obj, leaf_key, val)\n",
        "\n",
        "import math\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "\n",
        "class SiLU(nn.Module):\n",
        "   def forward(self, x):\n",
        "        return x*F.sigmoid(x)\n",
        "\n",
        "class NY(nn.Module):\n",
        "  def forward(self,x):\n",
        "    return 3*torch.tanh(0.3*x)\n",
        "\n",
        "class NewGELU(nn.Module):\n",
        "    def forward(self, x):\n",
        "        return 0.5 * x * (1.0 + torch.tanh(math.sqrt(2.0 / math.pi) * (x + 0.044715 * torch.pow(x, 3.0))))\n",
        "\n",
        "class SelfAttention(nn.Module):\n",
        "\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        assert config.n_embd % config.n_head == 0\n",
        "        self.c_attn = nn.Linear(config.n_embd, 3 * config.n_embd)\n",
        "        self.c_proj = nn.Linear(config.n_embd, config.n_embd)\n",
        "        self.attn_dropout = nn.Dropout(config.attn_pdrop)\n",
        "        self.resid_dropout = nn.Dropout(config.resid_pdrop)\n",
        "        self.register_buffer(\"bias\", torch.tril(torch.ones(config.block_size, config.block_size))\n",
        "                                     .view(1, 1, config.block_size, config.block_size))\n",
        "        self.n_head = config.n_head\n",
        "        self.n_embd = config.n_embd\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, T, C = x.size()\n",
        "        q, k ,v  = self.c_attn(x).split(self.n_embd, dim=2)\n",
        "        k = k.view(B, T, self.n_head, C // self.n_head).transpose(1, 2)\n",
        "        q = q.view(B, T, self.n_head, C // self.n_head).transpose(1, 2)\n",
        "        v = v.view(B, T, self.n_head, C // self.n_head).transpose(1, 2)\n",
        "        att = (q @ k.transpose(-2, -1)) * (1.0 / math.sqrt(k.size(-1)))\n",
        "        att = att.masked_fill(self.bias[:,:,:T,:T] == 0, float('-inf'))\n",
        "        att = F.softmax(att, dim=-1)\n",
        "        att = self.attn_dropout(att)\n",
        "        y = att @ v\n",
        "        y = y.transpose(1, 2).contiguous().view(B, T, C)\n",
        "        y = self.resid_dropout(self.c_proj(y))\n",
        "        return y\n",
        "\n",
        "class Block(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.ln_1 = nn.LayerNorm(config.n_embd)\n",
        "        self.attn = SelfAttention(config)\n",
        "        self.ln_2 = nn.LayerNorm(config.n_embd)\n",
        "        self.mlp = nn.ModuleDict(dict(\n",
        "            c_fc    = nn.Linear(config.n_embd, 4 * config.n_embd),\n",
        "            c_proj  = nn.Linear(4 * config.n_embd, config.n_embd),\n",
        "            act     = NewGELU(),\n",
        "            dropout = nn.Dropout(config.resid_pdrop),\n",
        "        ))\n",
        "        m = self.mlp\n",
        "        self.mlpf = lambda x: m.dropout(m.c_proj(m.act(m.c_fc(x))))\n",
        "    def forward(self, x):\n",
        "        x = x + self.attn(self.ln_1(x))\n",
        "        x = x + self.mlpf(self.ln_2(x))\n",
        "        return x\n",
        "\n",
        "class ClassifierI(nn.Module):\n",
        "    @staticmethod\n",
        "    def get_default_config():\n",
        "        C = CfgNode()\n",
        "        C.model_type = 'gpt'\n",
        "        C.n_layer = None\n",
        "        C.n_head = None\n",
        "        C.n_embd =  None\n",
        "        C.vocab_size = len(chars)\n",
        "        C.max_length = 512\n",
        "        C.embd_pdrop = 0.1\n",
        "        C.resid_pdrop = 0.1\n",
        "        C.attn_pdrop = 0.1\n",
        "        return C\n",
        "\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.max_length = config.block_size\n",
        "        self.soft = nn.Softmax(1)\n",
        "        self.config = self.get_default_config()\n",
        "        self.device = \"cpu\"\n",
        "        self.model_states = {'h':{'n_layer': 48, 'n_head': 25, 'n_embd': 1600},\n",
        "                                    'g':{'n_layer': 12, '': 12, 'n_embd': 768},\n",
        "                'f':   {'n_layer': 24, 'n_head': 16, 'n_embd': 1024},\n",
        "                'e':   {'n_layer': 36, 'n_head': 20, 'n_embd': 1280},\n",
        "                'd':{'n_layer': 8, 'n_head': 16, 'n_embd': 512},\n",
        "                'c':{'n_layer': 6, 'n_head': 6, 'n_embd': 192},\n",
        "                'b':{'n_layer': 4, 'n_head': 4, 'n_embd': 128},\n",
        "                'a':{'n_layer': 3, 'n_head': 3, 'n_embd': 48}}\n",
        "\n",
        "        type_ = config.model_type is not None\n",
        "        #assert type_ in \"abcdefgh\"\n",
        "        p = all([config.n_layer is not None, config.n_head is not None, config.n_embd is not None])\n",
        "        #assert type_ == True and p == True\n",
        "        if type_:\n",
        "            config.merge_from_dict(self.model_states[config.model_type])\n",
        "        self.closs = nn.BCELoss()\n",
        "        self.ny = NY()\n",
        "        #self.l = nn.Linear(512,1,64)\n",
        "        self.transformer = nn.ModuleDict(dict(\n",
        "            wte = nn.Embedding(config.vocab_size, config.n_embd),\n",
        "            wpe = nn.Embedding(config.max_length, config.n_embd),\n",
        "            drop = nn.Dropout(config.embd_pdrop),\n",
        "            h = nn.ModuleList([Block(config) for _ in range(config.n_layer)]),\n",
        "            ln_f = nn.LayerNorm(config.n_embd),))\n",
        "        self.classifier_head = nn.Sequential(#nn.Tanh(),\n",
        "                                             nn.Linear(config.n_embd, 2)\n",
        "                                             )\n",
        "        self.apply(self._init_weights)\n",
        "        for pn, p in self.named_parameters():\n",
        "            if pn.endswith('c_proj.weight'):\n",
        "                torch.nn.init.normal_(p, mean=0.0, std=0.02/math.sqrt(2 * config.n_layer))\n",
        "\n",
        "        n_params = sum(p.numel() for p in self.transformer.parameters())\n",
        "        #print(\"[ Number of trainable parameters: %.2fM ]\" % (n_params/1e6,))\n",
        "\n",
        "    def _init_weights(self, module):\n",
        "        if isinstance(module, nn.Linear):\n",
        "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
        "            if module.bias is not None:\n",
        "                torch.nn.init.zeros_(module.bias)\n",
        "        elif isinstance(module, nn.Embedding):\n",
        "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
        "        elif isinstance(module, nn.LayerNorm):\n",
        "            torch.nn.init.zeros_(module.bias)\n",
        "            torch.nn.init.ones_(module.weight)\n",
        "\n",
        "    @classmethod\n",
        "    def from_pretrained(cls, model_type):\n",
        "        from transformers import GPT2LMHeadModel\n",
        "        config = cls.get_default_config()\n",
        "        config.model_type = model_type\n",
        "        config.vocab_size = 25\n",
        "        config.max_length = 512\n",
        "        model = ClassifierI(config)\n",
        "        sd = model.state_dict()\n",
        "        model_hf = GPT2LMHeadModel.from_pretrained(model_type)\n",
        "        sd_hf = model_hf.state_dict()\n",
        "        keys = [k for k in sd_hf if not k.endswith('attn.masked_bias')]\n",
        "        transposed = ['attn.c_attn.weight', 'attn.c_proj.weight', 'mlp.c_fc.weight', 'mlp.c_proj.weight']\n",
        "        assert len(keys) == len(sd)\n",
        "        for k in keys:\n",
        "            if any(k.endswith(w) for w in transposed):\n",
        "                assert sd_hf[k].shape[::-1] == sd[k].shape\n",
        "                with torch.no_grad():\n",
        "                    sd[k].copy_(sd_hf[k].t())\n",
        "            else:\n",
        "                assert sd_hf[k].shape == sd[k].shape\n",
        "                with torch.no_grad():\n",
        "                    sd[k].copy_(sd_hf[k])\n",
        "        return model\n",
        "\n",
        "    def configure_optimizers(self, train_config):\n",
        "        decay = set()\n",
        "        no_decay = set()\n",
        "        whitelist_weight_modules = (torch.nn.Linear, )\n",
        "        blacklist_weight_modules = (torch.nn.LayerNorm, torch.nn.Embedding)\n",
        "        for mn, m in self.named_modules():\n",
        "            for pn, p in m.named_parameters():\n",
        "                fpn = '%s.%s' % (mn, pn) if mn else pn\n",
        "                if pn.endswith('bias'):\n",
        "                    no_decay.add(fpn)\n",
        "                elif pn.endswith('weight') and isinstance(m, whitelist_weight_modules):\n",
        "                    decay.add(fpn)\n",
        "                elif pn.endswith('weight') and isinstance(m, blacklist_weight_modules):\n",
        "                    no_decay.add(fpn)\n",
        "        param_dict = {pn: p for pn, p in self.named_parameters()}\n",
        "        inter_params = decay & no_decay\n",
        "        union_params = decay | no_decay\n",
        "        assert len(inter_params) == 0, \"parameters %s made it into both decay/no_decay sets!\" % (str(inter_params), )\n",
        "        assert len(param_dict.keys() - union_params) == 0, \"parameters %s were not separated into either decay/no_decay set!\" \\\n",
        "                                                    % (str(param_dict.keys() - union_params), )\n",
        "        optim_groups = [\n",
        "            {\"params\": [param_dict[pn] for pn in sorted(list(decay))], \"weight_decay\": train_config.weight_decay},\n",
        "            {\"params\": [param_dict[pn] for pn in sorted(list(no_decay))], \"weight_decay\": 0.0},]\n",
        "        optimizer = torch.optim.AdamW(optim_groups, lr=train_config.learning_rate, betas=train_config.betas)\n",
        "        return optimizer\n",
        "\n",
        "    @torch.no_grad\n",
        "    def predict_proba(self,idx,j =\"None\"):\n",
        "      sigmoid = nn.Sigmoid()\n",
        "      soft = nn.Softmax(dim=1)\n",
        "      ny = NY()\n",
        "      si = SiLU()\n",
        "      self.eval()\n",
        "      x,_ = self.forward(idx)\n",
        "      if j == \"None\":\n",
        "        return x[:,0:1]\n",
        "      elif j == \"soft\":\n",
        "        return self.soft(x)[:,0:1]\n",
        "      else:\n",
        "        return sigmoid(x)[:,0:1]\n",
        "\n",
        "    @torch.no_grad\n",
        "    def predict(self,idx):\n",
        "      g = torch.zeros((idx.shape[0],1))\n",
        "      e = -1\n",
        "      for i in self.predict_proba(idx):\n",
        "        e+=1\n",
        "        if i[0].item()>=0.5:\n",
        "          g[e] = 1\n",
        "      return g\n",
        "\n",
        "    def forward(self, idx, targets=None):\n",
        "        device = idx.device\n",
        "        b, t = idx.size()\n",
        "        pos = torch.arange(0, t, dtype=torch.long, device=device).unsqueeze(0)\n",
        "        tok_emb = self.transformer.wte(idx)\n",
        "        pos_emb = self.transformer.wpe(pos)\n",
        "        x = self.transformer.drop(tok_emb + pos_emb)\n",
        "        for block in self.transformer.h:\n",
        "            x = block(x)\n",
        "        x = self.transformer.ln_f(x)\n",
        "        logits = self.ny(self.classifier_head(x)).mean(1).to(device)\n",
        "        #logits = self.l(logits)\n",
        "        # = F.sigmoid(logits.view(b,2).mean(1).view(b,1))\n",
        "        loss = None\n",
        "        if targets is not None:\n",
        "            loss = F.cross_entropy(logits,targets,ignore_index = -1)\n",
        "        return logits, loss\n",
        "\n",
        "\n",
        "class Trainer:\n",
        "    @staticmethod\n",
        "    def get_default_config():\n",
        "        C = CfgNode()\n",
        "        C.device = 'cuda'\n",
        "        C.num_workers = 4\n",
        "        C.max_iters = None\n",
        "        C.batch_size = 64\n",
        "        C.max_length = 512\n",
        "        C.learning_rate = 8e-4\n",
        "        C.betas = (0.9, 0.95)\n",
        "        C.weight_decay = 0.1\n",
        "        C.grad_norm_clip = 1.0\n",
        "        return C\n",
        "\n",
        "    def __init__(self, config, model, train_dataset,val_dataset):\n",
        "        self.config = config\n",
        "        self.model = model\n",
        "        self.optimizer = None\n",
        "        self.train_dataset = train_dataset\n",
        "        self.val_dataset = val_dataset\n",
        "        self.callbacks = defaultdict(list)\n",
        "        self.device ='cuda'\n",
        "        self.model = self.model.to(self.device)\n",
        "        self.iter_num = 0\n",
        "        self.iter_time = 0.0\n",
        "        self.iter_dt = 0.0\n",
        "\n",
        "    def add_callback(self, onevent: str, callback):\n",
        "        self.callbacks[onevent].append(callback)\n",
        "\n",
        "    def set_callback(self, onevent: str, callback):\n",
        "        self.callbacks[onevent] = [callback]\n",
        "\n",
        "    def trigger_callbacks(self, onevent: str):\n",
        "        for callback in self.callbacks.get(onevent, []):\n",
        "            callback(self)\n",
        "\n",
        "    def run(self):\n",
        "        model, config = self.model, self.config\n",
        "        self.optimizer = model.configure_optimizers(config)\n",
        "        batch_size = self.config.batch_size\n",
        "        def get_batch(mode):\n",
        "            batch_size = 64\n",
        "            if mode == \"train\":\n",
        "              data = train_dataset\n",
        "              pos_train = torch.tensor(np.array([data[i:i+1,:] for i in range(data.shape[0]) if data[i,-1].item() == 1]).reshape(len([data[i:i+1,:] for i in range(data.shape[0]) if data[i,-1].item() == 1]),513))\n",
        "              neg_train = torch.tensor(np.array([data[i:i+1,:] for i in range(data.shape[0]) if data[i,-1].item() == 0]).reshape(len([data[i:i+1,:] for i in range(data.shape[0]) if data[i,-1].item() == 0]),513))\n",
        "              N = np.random.randint(batch_size)\n",
        "              ix_pos = np.random.randint(pos_train.shape[0]-N)\n",
        "              ix_neg = np.random.randint(neg_train.shape[0]-batch_size-N)\n",
        "              pos_data = pos_train[ix_pos:ix_pos+N,:]\n",
        "              neg_data = neg_train[ix_neg:ix_neg+batch_size-N,:]\n",
        "              data = torch.concat((pos_data,neg_data))\n",
        "              seq = data[:,:-1]\n",
        "              targets = torch.zeros((batch_size,2), device=self.device)\n",
        "              o = -1\n",
        "              for i in data:\n",
        "                o+=1\n",
        "                if i[-1].item() == 1:\n",
        "                  targets[o][0] = 1\n",
        "                else:\n",
        "                  targets[o][1] = 1\n",
        "            else:\n",
        "              data = val_dataset\n",
        "              ix = np.random.randint(data.shape[0]-batch_size)\n",
        "              data = data[ix:ix+batch_size,:]\n",
        "              seq = data[:,:-1]\n",
        "              targets = torch.zeros((batch_size,2), device=self.device)\n",
        "              o = -1\n",
        "              for i in data:\n",
        "                o+=1\n",
        "                if i[-1].item() == 1:\n",
        "                  targets[o][0] = 1\n",
        "                else:\n",
        "                  targets[o][1] = 1\n",
        "            targets = targets.view(batch_size,2).to(self.device)\n",
        "            seq = seq.view(batch_size,512).to(self.device)\n",
        "            return seq, targets\n",
        "\n",
        "        @torch.no_grad\n",
        "        def cross_val():\n",
        "          model.eval()\n",
        "          out = []\n",
        "          for i in [\"train\",\"val\"]:\n",
        "            losses = torch.zeros(200+1)\n",
        "            for k in range(200):\n",
        "              X,Y = get_batch(i)\n",
        "              logits,loss = model(X,Y)\n",
        "              losses[k]=loss.item()\n",
        "              out1 = losses.mean()\n",
        "            out.append(out1)\n",
        "          model.train()\n",
        "          return out\n",
        "        losses = cross_val()\n",
        "        LOSS = [losses]\n",
        "        print(\"\\n[train loss = {k}, val loss =  {j}]\\n\".format(k = losses[0],j = losses[1]))\n",
        "        model.train()\n",
        "        for epoch in range(20):\n",
        "          print(\"[epoch {o}] \\n\".format(o = epoch))\n",
        "          iters = 200\n",
        "          for i in range(iters):\n",
        "            if i == 0:\n",
        "              Y = \"| =\"\n",
        "            elif i == iters -1 :\n",
        "              Y = \"=> 100% |\"\n",
        "            else:\n",
        "              if i%(int(iters/50)) == 0 :\n",
        "                Y = \"=\"\n",
        "              else:\n",
        "                Y = \"\"\n",
        "            print(\"{y}\".format(y = Y),end=\"\")\n",
        "            x, y = get_batch(\"train\")\n",
        "            logits, self.loss = model(x, y)\n",
        "            model.zero_grad(set_to_none=True)\n",
        "            self.loss.backward()\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), config.grad_norm_clip)\n",
        "            self.optimizer.step()\n",
        "          losses = cross_val()\n",
        "          LOSS.append(losses)\n",
        "          print(LOSS)\n",
        "          print(\"\\n[train loss = {k}, val loss =  {j}]\\n\".format(k = losses[0],j = losses[1]))\n",
        "        PATH = \"model {h}\".format(h = model_config.model_type)\n",
        "        torch.save(model.state_dict(), PATH)\n",
        "class Transformer():\n",
        "  def __init__(self,mode):\n",
        "    self.mode = mode\n",
        "    self.device = device\n",
        "    self.vocab_dict = tokenizer.get_vocab()\n",
        "\n",
        "  def classifier(self):\n",
        "    device = self.device\n",
        "    model_config = ClassifierI.get_default_config()\n",
        "    model_config.vocab_size = 25\n",
        "    model_config.block_size = 512\n",
        "    if self.mode == \"b\":\n",
        "      model_config.model_type = 'b'\n",
        "      model2 = ClassifierI(model_config)\n",
        "      model2.load_state_dict(torch.load(\"Phaseek/model_b\",map_location=self.device,weights_only = True))\n",
        "    else:\n",
        "      model_config.model_type = 'c'\n",
        "      model2 = ClassifierI(model_config)\n",
        "      model2.load_state_dict(torch.load(\"/content/Phaseek/model/Core_model_1_5M_10_FEGS_features.pt\",map_location = self.device,weights_only = True))\n",
        "    model2.to(device)\n",
        "    return model2\n",
        "\n",
        "  def Encode(self, i):\n",
        "        def encode_char(x):\n",
        "            return [self.vocab_dict[char] for char in x]\n",
        "\n",
        "        def pad_sequences(x, PAD=0, max_len=512):\n",
        "            return np.array([seq + [PAD] * (max_len - len(seq)) for seq in x])\n",
        "\n",
        "        encoded = list(map(encode_char, i))\n",
        "        padded = pad_sequences(encoded)\n",
        "        return torch.tensor(padded).to(self.device)\n",
        "\n",
        "  def Decode(self,i):\n",
        "    def decode(k):\n",
        "      l = [j for j in k if j != 0 ]\n",
        "      seq = [chars[j] for j in l]\n",
        "      return \"\".join(seq)\n",
        "    H = list(map(decode,i))\n",
        "    G = []\n",
        "    for i in H:\n",
        "      if len(i)>512:\n",
        "        u = H.index(i)\n",
        "        for j in range(len(i)-512):\n",
        "          G.append(i[j:j+512])\n",
        "      else:\n",
        "        G.append(i)\n",
        "    return G,u\n",
        "  def Mean(self,i):\n",
        "    I = list(i.ravel())\n",
        "    sig = lambda x:np.e**(0*x)\n",
        "    Sum = [sig(np.abs(x-int(len(I)/2))) for x in range(len(I))]\n",
        "    wei = np.array([Sum[i]/sum(Sum) for i in range(len(I))]).reshape(len(I),1)\n",
        "    return np.sum(wei*i)\n",
        "\n",
        "  def enhancer(self,x):\n",
        "    wei = np.array([np.exp((i[0]-1)/(i[0])) for i in x])\n",
        "    return np.sum(np.array([i[0] for i in x])*wei)/np.sum(wei)\n",
        "\n",
        "  def predict_proba(self,i):\n",
        "    if len(i[0])>512:\n",
        "       l = [self.Encode([i[0][j:j+512]]) for j in range(len(i[0])-512)]\n",
        "       if len(l)>700:\n",
        "        L = [torch.concat(tuple(l[i*700:(i+1)*700])) for i in range(len(l)//700)] + [torch.concat(tuple(l[(len(l)//700)*700:(len(l)//700)*700+len(l)%700]))]\n",
        "        T = [self.classifier().predict_proba(i,\"sig\").tolist() for i in L ]\n",
        "        return self.enhancer(np.concatenate(tuple([i for i in T])))\n",
        "       else:\n",
        "         t = torch.concat(tuple(l))\n",
        "         return self.enhancer(self.classifier().predict_proba(t,\"sig\").tolist())\n",
        "    else:\n",
        "       seq = self.Encode(i)\n",
        "       U = np.array(self.classifier().predict_proba(seq,\"sig\").tolist())\n",
        "       out = np.array(U)\n",
        "       return out.reshape(len(i),1)\n",
        "\n",
        "model2 = Transformer(\"b\")\n",
        "model3 = Transformer(\"c\")\n",
        "\n",
        "\n",
        "import functools as FUNC\n",
        "from numba import njit\n",
        "\n",
        "class PPA():\n",
        "    def __init__(self, seq, model):\n",
        "        self.model = model\n",
        "        self.seq = seq\n",
        "        self.len = len(self.seq)\n",
        "        self.idx = None\n",
        "\n",
        "    def attention_mask(self):\n",
        "        idx2 = self.splicer(self.seq)[1]\n",
        "        k = self.T()\n",
        "        mask = [[True if len(k[0]) > idx2[j] - i > -1 else False for i in range(len(k[j]))] for j in range(len(k))]\n",
        "        return mask\n",
        "\n",
        "    def mean(self, mask, spliced):\n",
        "        u = [np.mean(self.model.predict_proba([spliced[i][j] for j in range(len(spliced[i])) if mask[i][j]])) for i in trange(len(spliced))]\n",
        "        return u\n",
        "\n",
        "    def splice_scorer(self, i):\n",
        "        lmin = len(i) // 2\n",
        "        return [i[j:j + lmin] for j in range(len(i) - lmin + 1)]\n",
        "\n",
        "    def splicer(self, i):\n",
        "        idx = self.idx\n",
        "        o, Before = [], []\n",
        "        for j in range(len(i)):\n",
        "            n_b, n_a = j, len(i) - j - 1\n",
        "            if n_b <= idx // 2:\n",
        "                before = i[max(0, j - idx // 2):j]\n",
        "                after = i[j + 1:j + idx - len(before)]\n",
        "            elif n_a <= idx // 2:\n",
        "                before = i[j - idx + n_a + 1:j]\n",
        "                after = i[j + 1:]\n",
        "            else:\n",
        "                before = i[j - idx // 2:j]\n",
        "                after = i[j + 1:j + idx // 2 + 1]\n",
        "            o.append(before + i[j] + after)\n",
        "            Before.append(len(before))\n",
        "        return o, Before\n",
        "\n",
        "    def T(self):\n",
        "        return list(map(self.splice_scorer, self.splicer(self.seq)[0]))\n",
        "\n",
        "    def Out(self):\n",
        "        s = self.s\n",
        "        def d(x):\n",
        "            if s > 0.7:\n",
        "                return x * np.exp(-1.2 * (x - s))\n",
        "            else:\n",
        "                return x if x > 0.7 else x\n",
        "\n",
        "        return list(map(d, self.mean(self.attention_mask(), self.T())))\n",
        "def show(i):\n",
        "  values = np.array(i).reshape(1, len(i))\n",
        "  y = range(len(i))\n",
        "  Y = [(j,1) for j in range(len(i)) if np.mean([i[j],i[min((j+1),len(i)-1)],i[max(0,(j-1))]])>0.6]\n",
        "  Y2 = [(j,1) for j in range(len(i)) if np.mean([i[j],i[min((j+1),len(i)-1)],i[min((j+2),len(i)-1)],i[min((j+3),len(i)-1)],i[min((j+4),len(i)-1)],i[min((j+5),len(i)-1)],i[max(0,(j-1))],i[max(0,(j-2))],i[max(0,(j-3))],i[max(0,(j-4))],i[max(0,(j-5))]])>0.85]\n",
        "  threshold = 0.5\n",
        "  above_threshold = np.maximum(values - threshold, 0)\n",
        "  below_threshold = np.minimum(values, threshold)\n",
        "  fig, (ax3 , ax4) = plt.subplots(2,1,sharex = True,sharey = True)\n",
        "  ax3.bar(y, below_threshold.ravel(), 0.9, color=\"lightblue\")\n",
        "  ax3.bar(y, above_threshold.ravel(), 0.9, color=\"lightgreen\",\n",
        "               bottom=below_threshold.ravel())\n",
        "  ax4.broken_barh(Y, (0.7, 0.03),\n",
        "               facecolors=( 'red'))\n",
        "  ax4.plot([0., y[-1]],[0.715,0.715],\"-b\")\n",
        "  for k in Y:\n",
        "    ax4.plot([k[0],k[0]+1],[0.715,0.715],\"-r\")\n",
        "\n",
        "  ax4.text(0,0.8 , \"LLPS-susceptible regions\",fontsize = 20)\n",
        "  ax3.plot([0., y[-1]], [threshold, threshold], \"k--\")\n",
        "  ax4.set_ylim([0,1])\n",
        "  ax3.set_ylim([0,1])\n",
        "\n",
        "  plt.subplots_adjust(\n",
        "                    wspace=0.4,\n",
        "                    hspace=0)\n",
        "  fig.set_figwidth(20)\n",
        "  fig.set_figheight(15)\n",
        "  plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TfclMO-48uW5",
        "outputId": "83ec6c4f-83eb-44e4-b3f7-09e496dfb425",
        "cellView": "form"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Score : 0.4292734315511125\n"
          ]
        }
      ],
      "source": [
        "#@title Loading sequence , then hit `Runtime` -> `Run all`\n",
        "#@markdown - Load the sequence of your protein :\n",
        "Sequence = \"MGKKQNRKTGNSKTQSASPPPKERSSSPATEQSWMENDFDELREEGFLREDIQTKGKEVENFEKNLEECITRITNTEKCLKELMELKTKARELREECRSLRSRCDQLEERVSAMEDEMNEMKREGKFREKRIKRNEQSLQEIWDYVKRPNLRLIGVPESDVENGTKLENTLQDIIQENFPNLARQANVQIQEIQRTPQRYSSRRATPRHIIVRFTKVEMKEKMLRAAREKGRVTLKGKPIRLTADLSAETLQARREWGPIFNILKEKNFQPRISYPAKLSFISEGEIKYFIDKQMLRDFVTTRPALKELLKEALNMERNNRYQPLQNHAKMLEHHHHHH \" #@param {type:\"string\"}\n",
        "ID = None #@param {type:\"string\"}\n",
        "Directory = None #@param {type:\"string\"}\n",
        "#@markdown  - Use `End_sequence` to focus the scoring on sequences up to the specified endpoint\n",
        "\n",
        "End_sequence = 1000 #@param {type:\"slider\" , min : 0 , max: 1000 , step : 1}\n",
        "Plot = True #@param {type: \"boolean\"}\n",
        "\n",
        "#@markdown - Specify the path of `FASTA format file` of your sequences.\n",
        "Fasta_file = \"/content/TopoisomeraseII_18.fasta\" #@param {type:\"string\"}\n",
        "import pandas as pd\n",
        "#assert [Sequence == None , Fasta_file == None].count(True) == 1\n",
        "\n",
        "def edit(i):\n",
        "  for j in i:\n",
        "    if j in \"\\t\\nBJOUXZ, _@#$%^& *n({}[])\\/\\\\\":\n",
        "      i = i.replace(j,\"\")\n",
        "  return i\n",
        "def rescale_scores(scores, original_threshold=0.454253, desired_threshold=0.5):\n",
        "    return (scores-original_threshold)*(1-desired_threshold)/(1-original_threshold)+desired_threshold\n",
        "def enh(x):\n",
        "  k = 50\n",
        "  wei = np.array([1 / (1 + np.exp(-k * (2 * i - 1))) for i in x])\n",
        "  return np.sum(np.array(x)*wei)/np.sum(wei)\n",
        "def att(x):\n",
        "  k = 100\n",
        "  wei = np.array([1 / (1 + np.exp(k * (2 * i - 1))) for i in x])\n",
        "  return np.sum(np.array(x)*wei)/np.sum(wei)\n",
        "\n",
        "\n",
        "def SCORE(i):\n",
        "  ii = clf.predict_proba(i)[:,1][0]\n",
        "  TT = [j for j in i.ravel() if j != 0]\n",
        "  S = 0.3*i[0][-1]+0.4*enh(TT[:-1])+0.3*ii\n",
        "  return S\n",
        "def SW(s):\n",
        "  if len(s)> 700:\n",
        "    L= [i for i in  [s[i*700:(i+1)*700] for i in range(len(s)//700+1)] if len(i)>0]\n",
        "    H = []\n",
        "    for i in range(len(L)):\n",
        "      H.append(model3.predict_proba(L[i]))\n",
        "    return H\n",
        "  else:\n",
        "    return model3.predict_proba(s)\n",
        "\n",
        "\n",
        "def Score1(i,Sc):\n",
        "    i = i-1\n",
        "    if i<= L:\n",
        "      return sum([Sc[j] for j in range(i+1)])/(i+1)\n",
        "\n",
        "    elif L+1<=i<n-L:\n",
        "       return sum([Sc[j] for j in range(i-L,i+1)])/(L+1)\n",
        "\n",
        "    else:\n",
        "       u = (n-(i)+1)\n",
        "       return sum([Sc[j] for j in range(i-L,n-L+1)])/(u)\n",
        "Sequence = edit(Sequence)\n",
        "\n",
        "\n",
        "def d(x):\n",
        "    if u>0.7:\n",
        "        return x*np.exp(-1.2*(x-u))\n",
        "    else:\n",
        "        if x > 0.7 :\n",
        "            return x\n",
        "        else:\n",
        "            return x\n",
        "def padd(i):\n",
        "  i = i+[float(0)]*(5537-len(i))\n",
        "  return i\n",
        "\n",
        "if Sequence != None and Sequence != \"\":\n",
        "  Sequence = edit(Sequence)\n",
        "  k = max(5,min(50,int(np.ceil(0.1*len(Sequence)))))\n",
        "  L = k//2\n",
        "  if Plot == True:\n",
        "    n = len(Sequence)\n",
        "    S = [Sequence[i:i+L] for i in range(len(Sequence)-L+1)]\n",
        "    Sc = SW(S)\n",
        "    Sc1 = Sc[0]\n",
        "    for i in Sc[1:]:\n",
        "      Sc1 = np.concatenate((Sc1,i))\n",
        "    u = model3.predict_proba([Sequence])\n",
        "\n",
        "    p = list(map(d,[Score1(i,Sc1) for i in range(1,n+1)]))\n",
        "    if type(p[0]) != np.float64:\n",
        "      p = [i[0] for i in p]\n",
        "    if type(p[0]) != np.float64:\n",
        "      p = [i[0] for i in p]\n",
        "\n",
        "    i = p\n",
        "    values = np.array(i).reshape(1, len(i))\n",
        "    below_threshold = [k if k<= 0.5 else 0.5 for k in i]\n",
        "    above_threshold = [0 if k<= 0.5 else k-0.5 for k in i]\n",
        "    threshold = 0.5\n",
        "    y = list(range(len(i)))\n",
        "    Y = [(j, 1) for j in range(len(i)) if np.mean([i[j], i[min((j + 1), len(i) - 1)], i[max(0, (j - 1))]]) > 0.6]\n",
        "    Y2 = [(j, 1) for j in range(len(i)) if np.mean([i[j], i[min((j + 1), len(i) - 1)], i[min((j + 2), len(i) - 1)],\n",
        "                                               i[min((j + 3), len(i) - 1)], i[min((j + 4), len(i) - 1)],\n",
        "                                               i[min((j + 5), len(i) - 1)], i[max(0, (j - 1))],\n",
        "                                               i[max(0, (j - 2))], i[max(0, (j - 3))], i[max(0, (j - 4))],\n",
        "                                               i[max(0, (j - 5))]]) > 0.85]\n",
        "    #if n>= 1500:\n",
        "      #c1 = \"blue\"\n",
        "      #c2 = \"green\"\n",
        "    #else:\n",
        "    c1 = \"lightblue\"\n",
        "    c2 = \"lightgreen\"\n",
        "\n",
        "    fig1 = make_subplots(rows=1, cols=1, shared_xaxes=True, shared_yaxes=True, vertical_spacing=0.02)\n",
        "    fig1.add_trace(go.Bar(x=y, y=below_threshold, marker_color= c1, name='Below Threshold', width= 0.6), row=1, col=1)\n",
        "    fig1.add_trace(go.Bar(x=y, y=above_threshold, marker_color=c2, name='Above Threshold', base=below_threshold, width= 0.6), row=1, col=1)\n",
        "\n",
        "    fig1.add_trace(go.Scatter(x=[0, len(i) - 1], y=[threshold, threshold], mode='lines', line=dict(color='black', dash='dash'), showlegend=False), row=1, col=1)\n",
        "    for start, duration in Y:\n",
        "        fig1.add_trace(go.Scatter(x=[start, start + duration], y=[0.01, 0.01], mode='lines', line=dict(color='red', width=7), showlegend=False), row=1, col=1)\n",
        "    fig1.update_layout(height=800, width=1700, title_text=\"LLPS Prediction Plot\")\n",
        "    if type(u) != np.float64:\n",
        "      u = u[0][0]\n",
        "    daa = np.array(padd(p)+[u]).reshape(1,5538)\n",
        "    score = str(SCORE(daa))\n",
        "    print(\"Score : \"+score)\n",
        "\n",
        "\n",
        "else:\n",
        "  if os.path.exists(Fasta_file):\n",
        "    with open(Fasta_file) as f:\n",
        "      U = f.read().split(\">\")\n",
        "      w = len(U)\n",
        "    #End_sequence = min(End_sequence,len(U))\n",
        "    j = U[:]\n",
        "    D = {\"id\":[i[:i.index(\"\\n\")] if \"\\n\" in i else i for i in j],\"seq\":[edit(i[i.index(\"\\n\"):]) if \"\\n\" in i else None for i in j],\"score\":[],\"ress_score\":[]}\n",
        "    #D[\"score\"] = [i[0][0] if type(i) != np.float64 else i for i in  [model3.predict_proba([s]) for s in tqdm(D[\"seq\"])]]\n",
        "    for sequence in tqdm(D[\"seq\"]):\n",
        "      try:\n",
        "        sequence = edit(sequence)\n",
        "        k = max(5,min(50,int(np.ceil(0.1*len(sequence)))))\n",
        "        L = k//2\n",
        "        n = len(sequence)\n",
        "        S = [sequence[i:i+L] for i in range(len(sequence)-L+1)]\n",
        "        Sc = SW(S)\n",
        "        Sc1 = Sc[0]\n",
        "        for i in Sc[1:]:\n",
        "          Sc1 = np.concatenate((Sc1,i))\n",
        "        u = model3.predict_proba([sequence])\n",
        "        p = list(map(d,[Score1(i,Sc1) for i in range(1,n+1)]))\n",
        "        if type(p[0]) != np.float64:\n",
        "          p = [i[0] for i in p]\n",
        "        if type(p[0]) != np.float64:\n",
        "          p = [i[0] for i in p]\n",
        "\n",
        "        if type(u) != np.float64:\n",
        "          u = u[0][0]\n",
        "        daa = np.array(padd(p)+[u]).reshape(1,5538)\n",
        "        score = str(SCORE(daa))\n",
        "        D[\"score\"].append(score)\n",
        "        D[\"ress_score\"].append(p)\n",
        "      except:\n",
        "        D[\"score\"].append(None)\n",
        "\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOdKAvZ8wZ3ZiRYYOLumTod",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}