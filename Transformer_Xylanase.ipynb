{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNdIA2xyJ9GcRB8W/XaKBE1",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AMIRMOHAMMAD-OSS/Bioinformatics/blob/main/Transformer_Xylanase.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install bio"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4NZMJAmi9_JH",
        "outputId": "7994a6f3-04b0-4eef-c557-625d9c766ce3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting bio\n",
            "  Downloading bio-1.7.1-py3-none-any.whl.metadata (5.7 kB)\n",
            "Collecting biopython>=1.80 (from bio)\n",
            "  Downloading biopython-1.84-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
            "Collecting gprofiler-official (from bio)\n",
            "  Downloading gprofiler_official-1.0.0-py3-none-any.whl.metadata (11 kB)\n",
            "Collecting mygene (from bio)\n",
            "  Downloading mygene-3.2.2-py2.py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from bio) (2.1.4)\n",
            "Requirement already satisfied: pooch in /usr/local/lib/python3.10/dist-packages (from bio) (1.8.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from bio) (2.32.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from bio) (4.66.5)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from biopython>=1.80->bio) (1.26.4)\n",
            "Collecting biothings-client>=0.2.6 (from mygene->bio)\n",
            "  Downloading biothings_client-0.3.1-py2.py3-none-any.whl.metadata (9.8 kB)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->bio) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->bio) (2024.1)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas->bio) (2024.1)\n",
            "Requirement already satisfied: platformdirs>=2.5.0 in /usr/local/lib/python3.10/dist-packages (from pooch->bio) (4.2.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from pooch->bio) (24.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->bio) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->bio) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->bio) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->bio) (2024.7.4)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->bio) (1.16.0)\n",
            "Downloading bio-1.7.1-py3-none-any.whl (280 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m281.0/281.0 kB\u001b[0m \u001b[31m12.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading biopython-1.84-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.2/3.2 MB\u001b[0m \u001b[31m79.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading gprofiler_official-1.0.0-py3-none-any.whl (9.3 kB)\n",
            "Downloading mygene-3.2.2-py2.py3-none-any.whl (5.4 kB)\n",
            "Downloading biothings_client-0.3.1-py2.py3-none-any.whl (29 kB)\n",
            "Installing collected packages: biopython, gprofiler-official, biothings-client, mygene, bio\n",
            "Successfully installed bio-1.7.1 biopython-1.84 biothings-client-0.3.1 gprofiler-official-1.0.0 mygene-3.2.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from Bio import SeqIO\n",
        "import torch"
      ],
      "metadata": {
        "id": "snFBjDiP-ESs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "GH11 = list(set([str(record.seq) for record in SeqIO.parse(\"/content/GH11_protein_sequences.fasta\", \"fasta\")]))\n",
        "GH10 = list(set([str(record.seq) for record in SeqIO.parse(\"/content/GH10_protein_sequences.fasta\", \"fasta\")]))\n",
        "GH12 = list(set([str(record.seq) for record in SeqIO.parse(\"/content/GH12_protein_sequences.fasta\", \"fasta\")]))\n",
        "GH9 = list(set([str(record.seq) for record in SeqIO.parse(\"/content/GH9_protein_sequences.fasta\", \"fasta\")]))\n",
        "GH8 = list(set([str(record.seq) for record in SeqIO.parse(\"/content/GH8_protein_sequences.fasta\", \"fasta\")]))"
      ],
      "metadata": {
        "id": "4gW29Cnx-PL0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pos = torch.concat((Encode(Padding([edit(i) for i in GH10 if len(i)<= 512])),torch.ones((5356,1))),1)\n",
        "neg = torch.concat((torch.concat((Encode(Padding([edit(i) for i in GH11 if len(i)<= 512])),Encode(Padding([edit(i) for i in GH12 if len(i)<= 512])),Encode(Padding([edit(i) for i in GH9 if len(i)<= 512])),Encode(Padding([edit(i) for i in GH8 if len(i)<= 512])))),torch.zeros((9346,1))),1)\n",
        "Dataset = torch.concat((pos,neg)).to(torch.long).to(\"cuda\")\n",
        "from sklearn.model_selection import train_test_split as TTS\n",
        "train_dataset,test_dataset = TTS(Dataset,random_state = 48 , test_size = 0.05)"
      ],
      "metadata": {
        "id": "Qdx9wXTo-e05"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Dataset = torch.concat((pos,neg)).to(torch.long).to(\"cuda\")"
      ],
      "metadata": {
        "id": "kp-C4LmNBrFw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split as TTS\n",
        "train_dataset,test_dataset = TTS(Dataset,random_state = 48 , test_size = 0.05)"
      ],
      "metadata": {
        "id": "76bMizl7BwFS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kAKGp_JtCAF5",
        "outputId": "e068cecf-c20a-4830-8b52-970e41167cf7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([736, 513])"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8BWGI0179UMn"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "def Padding(x,PAD = 0,max_len = 512):\n",
        "  l = []\n",
        "  max_len = 512\n",
        "  for i in x:\n",
        "    a = i\n",
        "    if len(a) < max_len:\n",
        "      a = a + \"X\"*(max_len-len(a))\n",
        "    l.append(a)\n",
        "  return l\n",
        "def edit(x):\n",
        "    for i in x:\n",
        "      if i in \"BJ\\tOUX\\Z_nabc\\ndefNghijklmnopqrstuvwxyz\":\n",
        "        x = x.replace(i,\"\")\n",
        "    return x\n",
        "def Encode(seqs):\n",
        "    cha = \"XACDEFGHIKLMNPQRSTVWY\"\n",
        "    return torch.vstack(tuple(torch.tensor([cha.index(i) for i in seq]) for seq in seqs)).to(torch.long).view(len(seqs), len(seqs[0]))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm import trange,tqdm\n",
        "import numpy as np\n",
        "from tqdm import trange,tqdm\n",
        "import numpy as np\n",
        "import torch\n",
        "from Bio import SeqIO\n",
        "import torch\n",
        "GH11 = list(set([str(record.seq) for record in SeqIO.parse(\"/content/GH11_protein_sequences.fasta\", \"fasta\")]))\n",
        "GH10 = list(set([str(record.seq) for record in SeqIO.parse(\"/content/GH10_protein_sequences.fasta\", \"fasta\")]))\n",
        "GH12 = list(set([str(record.seq) for record in SeqIO.parse(\"/content/GH12_protein_sequences.fasta\", \"fasta\")]))\n",
        "GH9 = list(set([str(record.seq) for record in SeqIO.parse(\"/content/GH9_protein_sequences.fasta\", \"fasta\")]))\n",
        "GH8 = list(set([str(record.seq) for record in SeqIO.parse(\"/content/GH8_protein_sequences.fasta\", \"fasta\")]))\n",
        "def Padding(x,PAD = 0,max_len = 512):\n",
        "  l = []\n",
        "  max_len = 512\n",
        "  for i in x:\n",
        "    a = i\n",
        "    if len(a) < max_len:\n",
        "      a = a + \"X\"*(max_len-len(a))\n",
        "    l.append(a)\n",
        "  return l\n",
        "def edit(x):\n",
        "    for i in x:\n",
        "      if i in \"BJ\\tOUX\\Z_nabc\\ndefNghijklmnopqrstuvwxyz\":\n",
        "        x = x.replace(i,\"\")\n",
        "    return x\n",
        "def Encode(seqs):\n",
        "    cha = \"XACDEFGHIKLMNPQRSTVWY\"\n",
        "    return torch.vstack(tuple(torch.tensor([cha.index(i) for i in seq]) for seq in seqs)).to(torch.long).view(len(seqs), len(seqs[0]))\n",
        "def set_seed(seed):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "def setup_logging(config):\n",
        "    work_dir = config.system.work_dir\n",
        "    os.makedirs(work_dir, exist_ok=True)\n",
        "    with open(os.path.join(work_dir, 'args.txt'), 'w') as f:\n",
        "        f.write(' '.join(sys.argv))\n",
        "    with open(os.path.join(work_dir, 'config.json'), 'w') as f:\n",
        "        f.write(json.dumps(config.to_dict(), indent=4))\n",
        "\n",
        "class CfgNode:\n",
        "    def __init__(self, **kwargs):\n",
        "        self.__dict__.update(kwargs)\n",
        "\n",
        "    def __str__(self):\n",
        "        return self._str_helper(0)\n",
        "\n",
        "    def _str_helper(self, indent):\n",
        "        parts = []\n",
        "        for k, v in self.__dict__.items():\n",
        "            if isinstance(v, CfgNode):\n",
        "                parts.append(\"%s:\\n\" % k)\n",
        "                parts.append(v._str_helper(indent + 1))\n",
        "            else:\n",
        "                parts.append(\"%s: %s\\n\" % (k, v))\n",
        "        parts = [' ' * (indent * 4) + p for p in parts]\n",
        "        return \"\".join(parts)\n",
        "\n",
        "    def to_dict(self):\n",
        "        return { k: v.to_dict() if isinstance(v, CfgNode) else v for k, v in self.__dict__.items() }\n",
        "\n",
        "    def merge_from_dict(self, d):\n",
        "        self.__dict__.update(d)\n",
        "\n",
        "    def merge_from_args(self, args):\n",
        "        for arg in args:\n",
        "            keyval = arg.split('=')\n",
        "            assert len(keyval) == 2, \"expecting each override arg to be of form --arg=value, got %s\" % arg\n",
        "            key, val = keyval\n",
        "            try:\n",
        "                val = literal_eval(val)\n",
        "            except ValueError:\n",
        "                pass\n",
        "\n",
        "            assert key[:2] == '--'\n",
        "            key = key[2:]\n",
        "            keys = key.split('.')\n",
        "            obj = self\n",
        "            for k in keys[:-1]:\n",
        "                obj = getattr(obj, k)\n",
        "            leaf_key = keys[-1]\n",
        "            assert hasattr(obj, leaf_key), f\"{key} is not an attribute that exists in the config\"\n",
        "            print(\"command line overwriting config attribute %s with %s\" % (key, val))\n",
        "            setattr(obj, leaf_key, val)\n",
        "import math\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "\n",
        "class SiLU(nn.Module):\n",
        "   def forward(self, x):\n",
        "        return x*F.sigmoid(x)\n",
        "\n",
        "class NY(nn.Module):\n",
        "  def forward(self,x):\n",
        "    return 3*torch.tanh(0.3*x)\n",
        "\n",
        "class NewGELU(nn.Module):\n",
        "    def forward(self, x):\n",
        "        return 0.5 * x * (1.0 + torch.tanh(math.sqrt(2.0 / math.pi) * (x + 0.044715 * torch.pow(x, 3.0))))\n",
        "\n",
        "class SelfAttention(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        assert config.n_embd % config.n_head == 0\n",
        "        self.c_attn = nn.Linear(config.n_embd, 3 * config.n_embd)\n",
        "        self.c_proj = nn.Linear(config.n_embd, config.n_embd)\n",
        "        self.attn_dropout = nn.Dropout(config.attn_pdrop)\n",
        "        self.resid_dropout = nn.Dropout(config.resid_pdrop)\n",
        "        self.register_buffer(\"bias\", torch.tril(torch.ones(config.block_size, config.block_size))\n",
        "                                     .view(1, 1, config.block_size, config.block_size))\n",
        "        self.n_head = config.n_head\n",
        "        self.n_embd = config.n_embd\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, T, C = x.size()\n",
        "        q, k ,v  = self.c_attn(x).split(self.n_embd, dim=2)\n",
        "        k = k.view(B, T, self.n_head, C // self.n_head).transpose(1, 2)\n",
        "        q = q.view(B, T, self.n_head, C // self.n_head).transpose(1, 2)\n",
        "        v = v.view(B, T, self.n_head, C // self.n_head).transpose(1, 2)\n",
        "        att = (q @ k.transpose(-2, -1)) * (1.0 / math.sqrt(k.size(-1)))\n",
        "        att = att.masked_fill(self.bias[:,:,:T,:T] == 0, float('-inf'))\n",
        "        att = F.softmax(att, dim=-1)\n",
        "        att = self.attn_dropout(att)\n",
        "        y = att @ v\n",
        "        y = y.transpose(1, 2).contiguous().view(B, T, C)\n",
        "        y = self.resid_dropout(self.c_proj(y))\n",
        "        return y\n",
        "\n",
        "class Block(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.ln1 = nn.LayerNorm(config.n_embd)\n",
        "        self.attn = nn.MultiheadAttention(config.n_embd, config.n_head, dropout=config.attn_pdrop)\n",
        "        self.ln2 = nn.LayerNorm(config.n_embd)\n",
        "        self.mlp = nn.Sequential(\n",
        "            nn.Linear(config.n_embd, 4 * config.n_embd),\n",
        "            nn.GELU(),\n",
        "            nn.Linear(4 * config.n_embd, config.n_embd),\n",
        "            nn.Dropout(config.resid_pdrop),\n",
        "        )\n",
        "\n",
        "    def forward(self, x, attention_mask=None):\n",
        "        if attention_mask is not None:\n",
        "            attention_mask = attention_mask.transpose(0, 1)\n",
        "\n",
        "        x = x + self.attn(self.ln1(x), self.ln1(x), self.ln1(x), key_padding_mask=attention_mask)[0]\n",
        "        x = x + self.mlp(self.ln2(x))\n",
        "        return x\n",
        "\n",
        "\n",
        "\n",
        "class ClassifierI(nn.Module):\n",
        "    @staticmethod\n",
        "    def get_default_config():\n",
        "        C = CfgNode()\n",
        "        C.model_type = 'gpt'\n",
        "        C.n_layer = None\n",
        "        C.n_head = None\n",
        "        C.n_embd =  None\n",
        "        C.vocab_size = 21\n",
        "        C.max_length = 512\n",
        "        C.embd_pdrop = 0.1\n",
        "        C.resid_pdrop = 0.1\n",
        "        C.attn_pdrop = 0.1\n",
        "        return C\n",
        "\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.max_length = config.block_size\n",
        "        self.soft = nn.Softmax(1)\n",
        "        self.config = self.get_default_config()\n",
        "        self.device = \"cuda\"\n",
        "        self.model_states = {\n",
        "            'h': {'n_layer': 48, 'n_head': 25, 'n_embd': 1600},\n",
        "            'g': {'n_layer': 12, 'n_head': 12, 'n_embd': 768},\n",
        "            'f': {'n_layer': 24, 'n_head': 16, 'n_embd': 1024},\n",
        "            'e': {'n_layer': 36, 'n_head': 20, 'n_embd': 1280},\n",
        "            'd': {'n_layer': 8, 'n_head': 16, 'n_embd': 512},\n",
        "            'c': {'n_layer': 6, 'n_head': 6, 'n_embd': 192},\n",
        "            'b': {'n_layer': 4, 'n_head': 4, 'n_embd': 128},\n",
        "            'a': {'n_layer': 3, 'n_head': 3, 'n_embd': 48}\n",
        "        }\n",
        "\n",
        "        type_ = config.model_type is not None\n",
        "        p = all([config.n_layer is not None, config.n_head is not None, config.n_embd is not None])\n",
        "        if type_:\n",
        "            config.merge_from_dict(self.model_states[config.model_type])\n",
        "\n",
        "        self.loss_fn = nn.BCEWithLogitsLoss()\n",
        "        self.ny = NY()\n",
        "        self.soft = nn.Softmax()\n",
        "        self.transformer = nn.ModuleDict(dict(\n",
        "            wte = nn.Embedding(config.vocab_size, config.n_embd),\n",
        "            wpe = nn.Embedding(config.max_length, config.n_embd),\n",
        "            drop = nn.Dropout(config.embd_pdrop),\n",
        "            h = nn.ModuleList([Block(config) for _ in range(config.n_layer)]),\n",
        "            ln_f = nn.LayerNorm(config.n_embd),\n",
        "        ))\n",
        "        self.classifier_head = nn.Sequential(\n",
        "            nn.Linear(config.n_embd, 2)\n",
        "        )\n",
        "        self.apply(self._init_weights)\n",
        "        for pn, p in self.named_parameters():\n",
        "            if pn.endswith('c_proj.weight'):\n",
        "                torch.nn.init.normal_(p, mean=0.0, std=0.02 / math.sqrt(2 * config.n_layer))\n",
        "\n",
        "        n_params = sum(p.numel() for p in self.transformer.parameters())\n",
        "        print(\"[ Number of trainable parameters: %.2fM ]\" % (n_params / 1e6,))\n",
        "\n",
        "    def forward(self, idx, attention_mask=None, targets=None):\n",
        "        device = idx.device\n",
        "        b, t = idx.size()\n",
        "        pos = torch.arange(0, t, dtype=torch.long, device=device).unsqueeze(0)\n",
        "        tok_emb = self.transformer.wte(idx)\n",
        "        pos_emb = self.transformer.wpe(pos)\n",
        "        x = self.transformer.drop(tok_emb + pos_emb)\n",
        "        for block in self.transformer.h:\n",
        "            x = block(x, attention_mask=attention_mask)\n",
        "        x = self.transformer.ln_f(x)\n",
        "        logits = self.soft(self.classifier_head(x).mean(1).to(device))\n",
        "        loss = None\n",
        "        if targets is not None:\n",
        "            loss = self.loss_fn(logits, targets)\n",
        "        return logits, loss\n",
        "\n",
        "    def _init_weights(self, module):\n",
        "        if isinstance(module, nn.Linear):\n",
        "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
        "            if module.bias is not None:\n",
        "                torch.nn.init.zeros_(module.bias)\n",
        "        elif isinstance(module, nn.Embedding):\n",
        "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
        "        elif isinstance(module, nn.LayerNorm):\n",
        "            torch.nn.init.zeros_(module.bias)\n",
        "            torch.nn.init.ones_(module.weight)\n",
        "\n",
        "    @classmethod\n",
        "    def from_pretrained(cls, model_type):\n",
        "        from transformers import GPT2LMHeadModel\n",
        "        config = cls.get_default_config()\n",
        "        config.model_type = model_type\n",
        "        config.vocab_size = 25\n",
        "        config.max_length = 512\n",
        "        model = ClassifierI(config)\n",
        "        sd = model.state_dict()\n",
        "        model_hf = GPT2LMHeadModel.from_pretrained(model_type)\n",
        "        sd_hf = model_hf.state_dict()\n",
        "        keys = [k for k in sd_hf if not k.endswith('attn.masked_bias')]\n",
        "        transposed = ['attn.c_attn.weight', 'attn.c_proj.weight', 'mlp.c_fc.weight', 'mlp.c_proj.weight']\n",
        "        assert len(keys) == len(sd)\n",
        "        for k in keys:\n",
        "            if any(k.endswith(w) for w in transposed):\n",
        "                assert sd_hf[k].shape[::-1] == sd[k].shape\n",
        "                with torch.no_grad():\n",
        "                    sd[k].copy_(sd_hf[k].t())\n",
        "            else:\n",
        "                assert sd_hf[k].shape == sd[k].shape\n",
        "                with torch.no_grad():\n",
        "                    sd[k].copy_(sd_hf[k])\n",
        "        return model\n",
        "\n",
        "    def configure_optimizers(self, train_config):\n",
        "        decay = set()\n",
        "        no_decay = set()\n",
        "        whitelist_weight_modules = (torch.nn.Linear, )\n",
        "        blacklist_weight_modules = (torch.nn.LayerNorm, torch.nn.Embedding)\n",
        "        for mn, m in self.named_modules():\n",
        "            for pn, p in m.named_parameters():\n",
        "                fpn = '%s.%s' % (mn, pn) if mn else pn\n",
        "                if pn.endswith('bias'):\n",
        "                    no_decay.add(fpn)\n",
        "                elif pn.endswith('weight') and isinstance(m, whitelist_weight_modules):\n",
        "                    decay.add(fpn)\n",
        "                elif pn.endswith('weight') and isinstance(m, blacklist_weight_modules):\n",
        "                    no_decay.add(fpn)\n",
        "        param_dict = {pn: p for pn, p in self.named_parameters()}\n",
        "        inter_params = decay & no_decay\n",
        "        union_params = decay | no_decay\n",
        "        optim_groups = [\n",
        "            {\"params\": [param_dict[pn] for pn in sorted(list(decay))], \"weight_decay\": 0},\n",
        "            {\"params\": [param_dict[pn] for pn in sorted(list(no_decay))], \"weight_decay\": 0.0},]\n",
        "        optimizer = torch.optim.AdamW(optim_groups, lr=0.0008)\n",
        "        return optimizer\n",
        "\n",
        "    @torch.no_grad\n",
        "    def predict_proba(self,idx,attention_mask,j =\"None\"):\n",
        "      sigmoid = nn.Sigmoid()\n",
        "      soft = nn.Softmax(dim=1)\n",
        "      ny = NY()\n",
        "      si = SiLU()\n",
        "      self.eval()\n",
        "      l,n = idx.shape\n",
        "      x,_ = self(idx,attention_mask)\n",
        "      if j == \"None\":\n",
        "        return x[:,1]\n",
        "      elif j == \"soft\":\n",
        "        return self.soft(x)[:,0:1]\n",
        "      else:\n",
        "        return sigmoid(x)[:,0:1]\n",
        "\n",
        "    @torch.no_grad\n",
        "    def predict(self,idx,attention_mask):\n",
        "      y = self.predict_proba(idx,attention_mask,\"None\").view(1,idx.shape[0]).tolist()[0]\n",
        "      y_pred = [i>=0.5 for i in y]\n",
        "      return y_pred,\n",
        "\n",
        "\n",
        "from sklearn.model_selection import KFold\n",
        "import numpy as np\n",
        "import torch\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "class Trainer:\n",
        "    @staticmethod\n",
        "    def get_default_config():\n",
        "        C = CfgNode()\n",
        "        C.device = 'cuda'\n",
        "        C.num_workers = 4\n",
        "        C.max_iters = None\n",
        "        C.batch_size = 64\n",
        "        C.max_length = 512\n",
        "        C.learning_rate = 0.001\n",
        "        C.betas = (0.9, 0.95)\n",
        "        C.weight_decay = 0.1\n",
        "        C.grad_norm_clip = 1.0\n",
        "        C.patience = 3\n",
        "        return C\n",
        "\n",
        "    def __init__(self, config, model, train_data, val_data):\n",
        "        self.config = config\n",
        "        self.model = model\n",
        "        self.optimizer = None\n",
        "        self.train_data = train_data\n",
        "        self.val_data = val_data\n",
        "        self.device = \"cuda\"\n",
        "        self.model = self.model.to(self.device)\n",
        "        self.iter_num = 0\n",
        "        self.iter_time = 0.0\n",
        "        self.iter_dt = 0.0\n",
        "        self.best_val_loss = float('inf')\n",
        "        self.early_stop_counter = 0\n",
        "\n",
        "    def get_batch(self, data, batch_size):\n",
        "        indices = np.random.permutation(len(data))[:batch_size]\n",
        "        batch = data[indices]\n",
        "        seq = batch[:, :-1]\n",
        "        targets = batch[:, -1]\n",
        "        targets = torch.nn.functional.one_hot(torch.tensor(targets, dtype=torch.int64), num_classes=2).float().to(self.device)\n",
        "        seq = torch.tensor(seq, dtype=torch.long).to(self.device)\n",
        "        attention_masks = (seq != 0).float().to(self.device).view(batch_size, 512)\n",
        "        return seq, attention_masks, targets\n",
        "\n",
        "    def run(self):\n",
        "        self.optimizer = self.model.configure_optimizers(self.config)\n",
        "        batch_size = 64\n",
        "\n",
        "        for epoch in range(50):\n",
        "            self.model.train()\n",
        "            for _ in trange(100):\n",
        "                seq, attention_mask, targets = self.get_batch(self.train_data, batch_size)\n",
        "                logits, loss = self.model(seq, attention_mask, targets)\n",
        "                self.optimizer.zero_grad()\n",
        "                loss.backward()\n",
        "                torch.nn.utils.clip_grad_norm_(self.model.parameters(), self.config.grad_norm_clip)\n",
        "                self.optimizer.step()\n",
        "\n",
        "            train_loss, val_loss = self.evaluate()\n",
        "            print(f\"\\n[train loss = {train_loss}, val loss =  {val_loss}]\\n\")\n",
        "\n",
        "            if val_loss < self.best_val_loss:\n",
        "                self.best_val_loss = val_loss\n",
        "                self.early_stop_counter = 0\n",
        "            else:\n",
        "                self.early_stop_counter += 1\n",
        "                if self.early_stop_counter >= self.config.patience:\n",
        "                    print(f\"Early stopping at epoch {epoch}\")\n",
        "                    break\n",
        "\n",
        "            # Validate and print accuracy\n",
        "            test_preds, test_labels = self.predict_and_evaluate()\n",
        "            acc = accuracy_score(test_labels, test_preds)\n",
        "            print(f\"Validation accuracy = {acc}\")\n",
        "\n",
        "        return self.model, test_preds\n",
        "\n",
        "    def evaluate(self):\n",
        "        self.model.eval()\n",
        "        losses = {'train': [], 'val': []}\n",
        "        for mode in ['train', 'val']:\n",
        "            dataset = self.train_data if mode == 'train' else self.val_data\n",
        "            for _ in range(200):\n",
        "                seq, attention_mask, targets = self.get_batch(dataset, self.config.batch_size)\n",
        "                logits, loss = self.model(seq, attention_mask, targets)\n",
        "                losses[mode].append(loss.item())\n",
        "        self.model.train()\n",
        "        return np.mean(losses['train']), np.mean(losses['val'])\n",
        "\n",
        "    def predict_and_evaluate(self):\n",
        "        self.model.eval()\n",
        "        idx = self.val_data[:, :-1].to(self.device)\n",
        "        attention_masks = (idx != 0).float().to(self.device).view(idx.shape[0], 512)\n",
        "        test_preds = self.model.predict(idx, attention_masks)[0]\n",
        "        test_labels = self.val_data[:, -1].tolist()\n",
        "        return test_preds, test_labels\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def cross_validate(model_class, data, labels, config):\n",
        "    kf = KFold(n_splits=10, shuffle=True, random_state=42)\n",
        "    accuracies = []\n",
        "\n",
        "    for fold, (train_index, val_index) in enumerate(kf.split(data), 1):\n",
        "        train_data, val_data = data[train_index], data[val_index]\n",
        "        train_labels, val_labels = labels[train_index], labels[val_index]\n",
        "\n",
        "        model = model_class(config)\n",
        "        trainer = Trainer(config, model, train_data, val_data)\n",
        "        trained_model, test_preds = trainer.run()\n",
        "\n",
        "        acc = accuracy_score(val_labels, test_preds)\n",
        "        accuracies.append(acc)\n",
        "        print(f'Fold {fold} Accuracy: {acc:.4f}')\n",
        "\n",
        "    print(f'Average Accuracy: {np.mean(accuracies):.4f}')\n",
        "    return accuracies\n",
        "\n",
        "\n",
        "model_config = ClassifierI.get_default_config()\n",
        "model_config.model_type = 'b'\n",
        "model_config.vocab_size = 21\n",
        "model_config.block_size = 512\n",
        "\n",
        "model = ClassifierI(model_config)\n",
        "train_config = Trainer.get_default_config()\n",
        "train_config.learning_rate = 0.0008\n",
        "train_config.max_iters = 2000\n",
        "train_config.num_workers = 0\n",
        "train_config.patience = 5\n",
        "trainer = Trainer(train_config, model, train_dataset, test_dataset)\n",
        "\n"
      ],
      "metadata": {
        "id": "IDsyn8-D9uFG"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}