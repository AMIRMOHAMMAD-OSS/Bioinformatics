{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "cl0MYdA59ifO",
        "3RqThK5j9vBH"
      ],
      "gpuType": "T4",
      "mount_file_id": "1MB48Bz3OsYeNJmTY61Vg_Dse3NYqr7XF",
      "authorship_tag": "ABX9TyO/QKYq22Xj0TBjgI6kLIv8",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AMIRMOHAMMAD-OSS/Bioinformatics/blob/main/MV_VAE_V1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H_O13VU08yvD"
      },
      "outputs": [],
      "source": [
        "!pip install Bio"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!cd /content/aiupred\n",
        "!unzip /content/aiupred/aiupred.zip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5YdjEe9-81FO",
        "outputId": "ce526603-0947-46d9-b0a8-c0e86a6e2235"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  /content/aiupred/aiupred.zip\n",
            "   creating: data/\n",
            "  inflating: data/decoder.pt         \n",
            "  inflating: data/embedding.pt       \n",
            "   creating: __pycache__/\n",
            "  inflating: __pycache__/aiupred_lib.cpython-311.pyc  \n",
            "  inflating: __pycache__/aiupred_lib.cpython-310.pyc  \n",
            "  inflating: aiupred.py              \n",
            "  inflating: aiupred_lib.py          \n",
            "  inflating: readme.md               \n",
            "  inflating: requirements.txt        \n",
            "  inflating: test.fasta              \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Data processing"
      ],
      "metadata": {
        "id": "LaFleChl9eim"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from __future__ import annotations\n",
        "from transformers import AutoTokenizer, AutoModel\n",
        "\n",
        "\n",
        "import json\n",
        "from pathlib import Path\n",
        "from typing import List, Dict, Any, Optional, Tuple\n",
        "\n",
        "import logging\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from transformers import EsmModel, EsmTokenizer\n",
        "\n",
        "# ----------------------------\n",
        "# Logging\n",
        "# ----------------------------\n",
        "logging.basicConfig(level=logging.INFO)\n",
        "logger = logging.getLogger(\"nrps_parser\")\n",
        "\n",
        "# ----------------------------\n",
        "# IUPred & handcrafted features utilities\n",
        "# ----------------------------\n",
        "import aiupred_lib\n",
        "\n",
        "MIN_IUPRED_LEN = 11  # conservative minimum to avoid savgol issues\n",
        "\n",
        "\n",
        "def safe_json_load(x: Any) -> Optional[Any]:\n",
        "    try:\n",
        "        if pd.isna(x):\n",
        "            return None\n",
        "        return json.loads(x)\n",
        "    except Exception:\n",
        "        return None\n",
        "\n",
        "\n",
        "def calculate_sequence_charge(sequence: str) -> np.ndarray:\n",
        "    sequence = (sequence or \"\").upper()\n",
        "    definite = {\"D\": -1, \"E\": -1, \"K\": 1, \"R\": 1, \"N_term\": 1, \"C_term\": -1}\n",
        "    out = []\n",
        "    for i, aa in enumerate(sequence):\n",
        "        c = definite.get(aa, 0)\n",
        "        if i == 0:\n",
        "            c += definite[\"N_term\"]\n",
        "        elif i == len(sequence) - 1:\n",
        "            c += definite[\"C_term\"]\n",
        "        out.append(c)\n",
        "    return np.asarray(out, dtype=float)\n",
        "\n",
        "\n",
        "def aromatic_clustering(seq: str) -> float:\n",
        "    arom = {\"Y\", \"W\", \"F\"}\n",
        "    idxs = [i for i, aa in enumerate(seq or \"\") if aa in arom]\n",
        "    n = len(idxs)\n",
        "    if n <= 1:\n",
        "        return 0.0\n",
        "    s = 0.0\n",
        "    pairs = 0\n",
        "    for i in range(n - 1):\n",
        "        for j in range(i + 1, n):\n",
        "            d = abs(idxs[j] - idxs[i])\n",
        "            if d > 0:\n",
        "                s += 1.0 / d\n",
        "                pairs += 1\n",
        "    return float(s / max(pairs, 1))\n",
        "\n",
        "\n",
        "def PPII_propensity(seq: str) -> List[float]:\n",
        "    table = {\n",
        "        'I': 0.39, 'V': 0.39, 'L': 0.24, 'F': 0.17, 'C': 0.25, 'M': 0.36, 'A': 0.37, 'G': 0.13,\n",
        "        'T': 0.32, 'S': 0.24, 'W': 0.25, 'Y': 0.25, 'P': 1.00, 'H': 0.20, 'E': 0.42, 'Q': 0.53,\n",
        "        'D': 0.30, 'N': 0.27, 'K': 0.56, 'R': 0.38\n",
        "    }\n",
        "    return [table.get(a, 0.0) for a in (seq or \"\")]\n",
        "\n",
        "\n",
        "def safe_predict_iupred(seq: str, embedding_model=None, regression_model=None, device: str = \"cpu\") -> np.ndarray:\n",
        "    seq = str(seq or \"\")\n",
        "    if len(seq) < MIN_IUPRED_LEN:\n",
        "        return np.full(len(seq), 0.4, dtype=float)\n",
        "    try:\n",
        "        scores = aiupred_lib.predict_disorder(seq, embedding_model, regression_model, device)\n",
        "        scores = np.asarray(scores, dtype=float)\n",
        "        if scores.size == 0:\n",
        "            raise ValueError(\"empty iupred output\")\n",
        "        return scores\n",
        "    except Exception as e:\n",
        "        logger.warning(f\"IUPred failed (len={len(seq)}): {e}. Using fallback 0.4.\")\n",
        "        return np.full(len(seq), 0.4, dtype=float)\n",
        "\n",
        "\n",
        "def compute_handcrafted_features(\n",
        "    sequence: str,\n",
        "    region_type: str,\n",
        "    embedding_model=None,\n",
        "    regression_model=None,\n",
        "    device: str = \"cpu\",\n",
        "    dim: int = 15,\n",
        ") -> np.ndarray:\n",
        "    seq = str(sequence or \"\")\n",
        "    feats: List[float]\n",
        "\n",
        "    if region_type in [\"A-T\", \"T-C\"]:  # linkers\n",
        "        ius = safe_predict_iupred(seq, embedding_model, regression_model, device)\n",
        "        mean_iupred = float(ius.mean()) if ius.size else 0.0\n",
        "        charges = calculate_sequence_charge(seq)\n",
        "        mean_charge = float(charges.mean()) if charges.size else 0.0\n",
        "        pp2 = float(np.mean(PPII_propensity(seq))) if seq else 0.0\n",
        "        feats = [len(seq), mean_charge, mean_iupred, aromatic_clustering(seq), pp2]\n",
        "\n",
        "    elif region_type == \"A\":\n",
        "        # compact Stachelhaus-like one-hot for first 10 positions being hydrophobic\n",
        "        feats = [1.0 if i < len(seq) and seq[i] in \"AILV\" else 0.0 for i in range(10)]\n",
        "\n",
        "    elif region_type == \"T\":\n",
        "        feats = [1.0 if \"S\" in seq else 0.0]\n",
        "\n",
        "    elif region_type == \"C\":\n",
        "        charges = calculate_sequence_charge(seq)\n",
        "        mean_charge = float(charges.mean()) if charges.size else 0.0\n",
        "        feats = [1.0 if \"HH\" in seq else 0.0, mean_charge]\n",
        "\n",
        "    else:\n",
        "        feats = []\n",
        "\n",
        "    if len(feats) < dim:\n",
        "        feats += [0.0] * (dim - len(feats))\n",
        "    else:\n",
        "        feats = feats[:dim]\n",
        "\n",
        "    return np.asarray(feats, dtype=float)\n",
        "\n",
        "# ----------------------------\n",
        "# ESM2 (t33) and pooling\n",
        "# ----------------------------\n",
        "ESM2_MODEL_NAME = \"facebook/esm2_t33_650M_UR50D\"\n",
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "tokenizer = EsmTokenizer.from_pretrained(ESM2_MODEL_NAME)\n",
        "esm_model = EsmModel.from_pretrained(ESM2_MODEL_NAME).eval().to(DEVICE)\n",
        "HIDDEN = esm_model.config.hidden_size  # 1280 for t33\n",
        "\n",
        "\n",
        "class AttentionPooling(nn.Module):\n",
        "    def __init__(self, dim: int):\n",
        "        super().__init__()\n",
        "        self.attn = nn.Linear(dim, 1)\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        # x: (L, d)\n",
        "        scores = self.attn(x).squeeze(-1)          # (L,)\n",
        "        weights = torch.softmax(scores, dim=0)     # (L,)\n",
        "        pooled = torch.sum(weights.unsqueeze(-1) * x, dim=0)\n",
        "        return pooled  # (d,)\n",
        "\n",
        "\n",
        "attn_pool = AttentionPooling(HIDDEN).to(DEVICE)\n",
        "\n",
        "# Optional motif coordinates per region (within-region indices). Keep placeholders or empty.\n",
        "MOTIF_POS: Dict[str, List[int]] = {\n",
        "    \"A\": [],  # e.g., [235, 239, 278]\n",
        "    \"T\": [],  # e.g., [7]\n",
        "    \"C\": [],  # e.g., [120, 121]\n",
        "}\n",
        "\n",
        "\n",
        "@torch.no_grad()\n",
        "def esm2_pool(seq: str) -> torch.Tensor:\n",
        "    \"\"\"Return a fixed (HIDDEN,) ESM2 vector for a sequence via attention pooling.\n",
        "       Falls back to zeros for empty sequences.\n",
        "    \"\"\"\n",
        "    if not seq or not seq.strip():\n",
        "        return torch.zeros(HIDDEN, device=DEVICE)\n",
        "\n",
        "    toks = tokenizer(seq, return_tensors=\"pt\", add_special_tokens=True)\n",
        "    toks = {k: v.to(DEVICE) for k, v in toks.items()}\n",
        "    out = esm_model(**toks)\n",
        "    tok = out.last_hidden_state.squeeze(0)  # (L, d)\n",
        "    if tok.size(0) >= 2:\n",
        "        tok = tok[1:-1]  # drop CLS/EOS\n",
        "    pooled = attn_pool(tok)  # (d,)\n",
        "    return pooled\n",
        "\n",
        "\n",
        "@torch.no_grad()\n",
        "def motif_pool(emb: torch.Tensor, motif_positions: List[int]) -> torch.Tensor:\n",
        "    \"\"\"Average embeddings at motif_positions from per-token embeddings if available.\n",
        "       Here we recompute tokens to get per-token embeddings, then pool motifs.\n",
        "       If no motifs or OOB, returns zeros(HIDDEN).\n",
        "    \"\"\"\n",
        "    if motif_positions is None or len(motif_positions) == 0:\n",
        "        return torch.zeros(HIDDEN, device=DEVICE)\n",
        "    # emb here is already pooled; in this streamlined version we skip motif re-encoding\n",
        "    # To keep a fixed size and avoid a second forward, return zeros unless you modify to pass token-level emb.\n",
        "    return torch.zeros(HIDDEN, device=DEVICE)\n",
        "\n",
        "\n",
        "@torch.no_grad()\n",
        "def region_vector(\n",
        "    seq: str,\n",
        "    handcrafted: np.ndarray,\n",
        "    region_name: str,\n",
        "    concat_motif: bool = False,\n",
        ") -> np.ndarray:\n",
        "    \"\"\"Build a fixed-size vector per region: [handcrafted | esm_attn (| motif_emb)].\"\"\"\n",
        "    hf = torch.as_tensor(handcrafted, dtype=torch.float32, device=DEVICE)\n",
        "    esm_vec = esm2_pool(seq)  # (HIDDEN,)\n",
        "\n",
        "    if concat_motif:\n",
        "        motif_vec = motif_pool(esm_vec, MOTIF_POS.get(region_name, []))  # zeros for now\n",
        "        full = torch.cat([hf, esm_vec, motif_vec], dim=-1)\n",
        "    else:\n",
        "        full = torch.cat([hf, esm_vec], dim=-1)\n",
        "\n",
        "    return full.detach().cpu().numpy()\n",
        "\n",
        "\n",
        "# ----------------------------\n",
        "# Main parser that returns fixed-dim tensors\n",
        "# ----------------------------\n",
        "def parse_excel(\n",
        "    data_file: str | Path,\n",
        "    embedding_model=None,\n",
        "    regression_model=None,\n",
        "    device_iupred: str = \"cpu\",\n",
        "    feature_dim: int = 15,\n",
        "    concat_motif: bool = False,\n",
        ") -> Dict[str, Any]:\n",
        "    df = pd.read_excel(data_file)\n",
        "\n",
        "    # Keep rows with exactly one linker region between domains\n",
        "    df = df[df[\"Between_T_C_Domains\"].apply(lambda x: len(safe_json_load(x) or []) == 1)]\n",
        "    df = df[df[\"Between_A_T_Domains\"].apply(lambda x: len(safe_json_load(x) or []) == 1)]\n",
        "\n",
        "    modules: List[List[str]] = []\n",
        "    features_list: List[np.ndarray] = []\n",
        "    mask_list: List[np.ndarray] = []\n",
        "    labels: List[int] = []\n",
        "    bgc_ids: List[Any] = []\n",
        "\n",
        "    regions = [\"A\", \"A-T\", \"T\", \"T-C\", \"C\"]\n",
        "\n",
        "    for _, row in tqdm(df.iterrows(), total=len(df), desc=\"Parsing modules\"):\n",
        "        module_seqs = [\n",
        "            row[\"A_Sequence\"],\n",
        "            \"\",  # A-T linker\n",
        "            row[\"T_Sequence\"],\n",
        "            \"\",  # T-C linker\n",
        "            row[\"C_Sequence\"],\n",
        "        ]\n",
        "\n",
        "        # A-T linker\n",
        "        at_info = safe_json_load(row[\"Between_A_T_Domains\"])\n",
        "        module_seqs[1] = (at_info[0].get(\"Sequence\", \"\") if at_info else \"GS\" * 20)\n",
        "\n",
        "        # T-C linker\n",
        "        tc_info = safe_json_load(row[\"Between_T_C_Domains\"])\n",
        "        module_seqs[3] = (tc_info[0].get(\"Sequence\", \"\") if tc_info else \"PP\" * 20)\n",
        "\n",
        "        modules.append(module_seqs)\n",
        "        labels.append(1)\n",
        "        bgc_ids.append(row.get(\"BGC_ID\", None))\n",
        "\n",
        "        # Per-region vectors\n",
        "        region_vecs: List[np.ndarray] = []\n",
        "        region_mask: List[float] = []\n",
        "        for seq, reg in zip(module_seqs, regions):\n",
        "            handcrafted = compute_handcrafted_features(seq, reg, embedding_model, regression_model, device_iupred, feature_dim)\n",
        "            vec = region_vector(seq, handcrafted, reg, concat_motif=concat_motif)\n",
        "            region_vecs.append(vec)\n",
        "            # mask = 0 if both handcrafted and esm are zeros (rare), else 1\n",
        "            region_mask.append(0.0 if np.allclose(vec, 0.0) else 1.0)\n",
        "\n",
        "        # Ensure fixed shape per module\n",
        "        region_mat = np.stack(region_vecs, axis=0)     # (5, D_total)\n",
        "        features_list.append(region_mat)\n",
        "        mask_list.append(np.asarray(region_mask, dtype=float))\n",
        "\n",
        "    features = np.stack(features_list, axis=0)  # (N, 5, D_total)\n",
        "    mask = np.stack(mask_list, axis=0)         # (N, 5)\n",
        "\n",
        "    return {\n",
        "        \"sequences\": modules,\n",
        "        \"features\": features,\n",
        "        \"mask\": mask,\n",
        "        \"labels\": np.asarray(labels, dtype=int),\n",
        "        \"bgc_ids\": np.asarray(bgc_ids),\n",
        "        \"d_in\": features.shape[-1],\n",
        "    }\n",
        "\n",
        "\n",
        "# ----------------------------\n",
        "# Example run\n",
        "# ----------------------------\n",
        "if __name__ == \"__main__\":\n",
        "    # Initialize IUPred models (if you actually use IUPred)\n",
        "    emb_model, reg_model, iupred_device = aiupred_lib.init_models()\n",
        "\n",
        "    data_file = Path(\"/content/drive/MyDrive/NRPS-dataet/MiBIG/MiBIG_tridomains_extracted.xlsx\")\n",
        "    data = parse_excel(\n",
        "        data_file,\n",
        "        embedding_model=emb_model,\n",
        "        regression_model=reg_model,\n",
        "        device_iupred=iupred_device,\n",
        "        feature_dim=15,\n",
        "        concat_motif=False,  # set True if you implement real motif pooling\n",
        "    )\n",
        "\n",
        "    print(f\"Parsed {len(data['sequences'])} modules\")\n",
        "    print(f\"Feature tensor shape: {data['features'].shape}\")\n",
        "    print(f\"Mask shape: {data['mask'].shape}\")\n",
        "    print(f\"Per-region dim (d_in): {data['d_in']}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UpzuhWtx9Osi",
        "outputId": "280bcf76-dc0d-486b-a50f-e8e384b3596f"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of EsmModel were not initialized from the model checkpoint at facebook/esm2_t33_650M_UR50D and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Parsing modules: 100%|██████████| 1821/1821 [15:40<00:00,  1.94it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Parsed 1821 modules\n",
            "Feature tensor shape: (1821, 5, 1295)\n",
            "Mask shape: (1821, 5)\n",
            "Per-region dim (d_in): 1295\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Model"
      ],
      "metadata": {
        "id": "cl0MYdA59ifO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from __future__ import annotations\n",
        "import math\n",
        "import os\n",
        "from dataclasses import dataclass\n",
        "from typing import Optional, Dict, Any, Tuple\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "@dataclass\n",
        "class VAEConfig:\n",
        "    d_in: int\n",
        "    n_regions: int = 5\n",
        "    d_model: int = 256\n",
        "    n_heads: int = 8\n",
        "    n_layers: int = 2\n",
        "    d_ff: int = 512\n",
        "    dropout: float = 0.1\n",
        "\n",
        "    latent_dim: int = 64\n",
        "    beta: float = 1.0\n",
        "    kl_warmup_steps: int = 2000\n",
        "\n",
        "    decoder_hidden: int = 256\n",
        "    per_region_decoders: bool = False\n",
        "\n",
        "    lr: float = 1e-3\n",
        "    weight_decay: float = 1e-4\n",
        "    batch_size: int = 64\n",
        "    num_epochs: int = 30\n",
        "\n",
        "    use_probe: bool = False\n",
        "    probe_weight: float = 0.1\n",
        "\n",
        "    device: str = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "class NRPSDataset(Dataset):\n",
        "    def __init__(self, features: np.ndarray, mask: Optional[np.ndarray] = None):\n",
        "        \"\"\"\n",
        "        features: (N, 5, D)\n",
        "        mask: (N, 5) 1=present, 0=missing (optional). If None, assume all ones.\n",
        "        \"\"\"\n",
        "        assert features.ndim == 3\n",
        "        self.x = torch.from_numpy(features).float()\n",
        "        if mask is None:\n",
        "            self.m = torch.ones(self.x.shape[:2], dtype=torch.float32)\n",
        "        else:\n",
        "            self.m = torch.from_numpy(mask).float()\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.x.shape[0]\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.x[idx], self.m[idx]\n",
        "\n",
        "\n",
        "class SegmentPositionalEmbedding(nn.Module):\n",
        "    def __init__(self, n_regions: int, d_model: int):\n",
        "        super().__init__()\n",
        "        self.proj = nn.Linear(1, d_model)\n",
        "        self.segment = nn.Embedding(n_regions, d_model)\n",
        "        self.pos = nn.Embedding(n_regions, d_model)\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        B, R, _ = x.shape\n",
        "        seg_ids = torch.arange(R, device=x.device).unsqueeze(0).expand(B, R)\n",
        "        pos_ids = torch.arange(R, device=x.device).unsqueeze(0).expand(B, R)\n",
        "        return self.segment(seg_ids) + self.pos(pos_ids)\n",
        "\n",
        "\n",
        "class MultiRegionEncoder(nn.Module):\n",
        "    def __init__(self, cfg: VAEConfig):\n",
        "        super().__init__()\n",
        "        self.cfg = cfg\n",
        "        self.input_proj = nn.Linear(cfg.d_in, cfg.d_model)\n",
        "        enc_layer = nn.TransformerEncoderLayer(\n",
        "            d_model=cfg.d_model,\n",
        "            nhead=cfg.n_heads,\n",
        "            dim_feedforward=cfg.d_ff,\n",
        "            dropout=cfg.dropout,\n",
        "            batch_first=True,\n",
        "            activation=\"gelu\",\n",
        "        )\n",
        "        self.encoder = nn.TransformerEncoder(enc_layer, num_layers=cfg.n_layers)\n",
        "        self.segpos = SegmentPositionalEmbedding(cfg.n_regions, cfg.d_model)\n",
        "        self.layernorm = nn.LayerNorm(cfg.d_model)\n",
        "        self.cls_token = nn.Parameter(torch.randn(1, 1, cfg.d_model) * 0.02)\n",
        "        self.to_mu = nn.Linear(cfg.d_model, cfg.latent_dim)\n",
        "        self.to_logvar = nn.Linear(cfg.d_model, cfg.latent_dim)\n",
        "\n",
        "    def forward(self, x: torch.Tensor, mask: Optional[torch.Tensor] = None) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n",
        "        B, R, _ = x.shape\n",
        "        h = self.input_proj(x)\n",
        "        h = h + self.segpos(x)\n",
        "        cls = self.cls_token.expand(B, 1, -1)\n",
        "        h = torch.cat([cls, h], dim=1)\n",
        "\n",
        "        src_key_padding_mask = None\n",
        "        if mask is not None:\n",
        "            pad = (mask <= 0).bool()\n",
        "            pad = torch.cat([torch.zeros(B, 1, device=pad.device, dtype=torch.bool), pad], dim=1)\n",
        "            src_key_padding_mask = pad\n",
        "\n",
        "        h = self.encoder(h, src_key_padding_mask=src_key_padding_mask)\n",
        "        h_cls = self.layernorm(h[:, 0])\n",
        "        mu = self.to_mu(h_cls)\n",
        "        logvar = self.to_logvar(h_cls)\n",
        "        return mu, logvar, h[:, 1:]\n",
        "\n",
        "class MultiRegionDecoder(nn.Module):\n",
        "    def __init__(self, cfg: VAEConfig):\n",
        "        super().__init__()\n",
        "        self.cfg = cfg\n",
        "        self.from_z = nn.Sequential(\n",
        "            nn.Linear(cfg.latent_dim, cfg.decoder_hidden),\n",
        "            nn.GELU(),\n",
        "            nn.Linear(cfg.decoder_hidden, cfg.n_regions * cfg.d_model),\n",
        "            nn.GELU(),\n",
        "        )\n",
        "        if cfg.per_region_decoders:\n",
        "            self.decoders = nn.ModuleList(\n",
        "                [nn.Sequential(\n",
        "                    nn.Linear(cfg.d_model, cfg.decoder_hidden),\n",
        "                    nn.GELU(),\n",
        "                    nn.Linear(cfg.decoder_hidden, cfg.d_in)\n",
        "                ) for _ in range(self.cfg.n_regions)]\n",
        "            )\n",
        "        else:\n",
        "            self.shared = nn.Sequential(\n",
        "                nn.Linear(cfg.d_model, cfg.decoder_hidden),\n",
        "                nn.GELU(),\n",
        "                nn.Linear(cfg.decoder_hidden, cfg.d_in)\n",
        "            )\n",
        "\n",
        "    def forward(self, z: torch.Tensor) -> torch.Tensor:\n",
        "        B = z.size(0)\n",
        "        H = self.from_z(z)  # (B, R*d_model)\n",
        "        H = H.view(B, self.cfg.n_regions, self.cfg.d_model)\n",
        "        if self.cfg.per_region_decoders:\n",
        "            outs = []\n",
        "            for r in range(self.cfg.n_regions):\n",
        "                outs.append(self.decoders[r](H[:, r, :]))\n",
        "            x_hat = torch.stack(outs, dim=1)\n",
        "        else:\n",
        "            x_hat = self.shared(H.view(-1, self.cfg.d_model))\n",
        "            x_hat = x_hat.view(B, self.cfg.n_regions, self.cfg.d_in)\n",
        "        return x_hat\n",
        "\n",
        "class MultiInputVAE(nn.Module):\n",
        "    def __init__(self, cfg: VAEConfig):\n",
        "        super().__init__()\n",
        "        self.cfg = cfg\n",
        "        self.encoder = MultiRegionEncoder(cfg)\n",
        "        self.decoder = MultiRegionDecoder(cfg)\n",
        "        if cfg.use_probe:\n",
        "            self.probe = nn.Sequential(\n",
        "                nn.Linear(cfg.latent_dim, 64), nn.GELU(), nn.Linear(64, 1)\n",
        "            )\n",
        "        else:\n",
        "            self.probe = None\n",
        "\n",
        "    def reparameterize(self, mu: torch.Tensor, logvar: torch.Tensor) -> torch.Tensor:\n",
        "        std = torch.exp(0.5 * logvar)\n",
        "        eps = torch.randn_like(std)\n",
        "        return mu + eps * std\n",
        "\n",
        "    def forward(self, x: torch.Tensor, mask: Optional[torch.Tensor] = None) -> Dict[str, torch.Tensor]:\n",
        "        mu, logvar, _ = self.encoder(x, mask)\n",
        "        z = self.reparameterize(mu, logvar)\n",
        "        x_hat = self.decoder(z)\n",
        "        out = {\"x_hat\": x_hat, \"mu\": mu, \"logvar\": logvar, \"z\": z}\n",
        "        if self.probe is not None:\n",
        "            out[\"probe\"] = self.probe(z).squeeze(-1)\n",
        "        return out\n",
        "\n",
        "class ELBOLoss(nn.Module):\n",
        "    def __init__(self, cfg: VAEConfig):\n",
        "        super().__init__()\n",
        "        self.cfg = cfg\n",
        "\n",
        "    def forward(self, x: torch.Tensor, out: Dict[str, torch.Tensor], mask: Optional[torch.Tensor] = None,\n",
        "                step: int = 0, y: Optional[torch.Tensor] = None) -> Tuple[torch.Tensor, Dict[str, float]]:\n",
        "        x_hat, mu, logvar = out[\"x_hat\"], out[\"mu\"], out[\"logvar\"]\n",
        "        if mask is not None:\n",
        "            m = mask.unsqueeze(-1)  # (B, R, 1)\n",
        "            recon = F.mse_loss(x_hat * m, x * m, reduction=\"sum\") / (m.sum() + 1e-8)\n",
        "        else:\n",
        "            recon = F.mse_loss(x_hat, x, reduction=\"mean\")\n",
        "        kl = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp(), dim=1).mean()\n",
        "        beta = min(1.0, self.cfg.beta * (step / max(1, self.cfg.kl_warmup_steps)))\n",
        "        loss = recon + beta * kl\n",
        "        logs = {\"loss\": float(loss.item()), \"recon\": float(recon.item()), \"kl\": float(kl.item()), \"beta\": float(beta)}\n",
        "        if (y is not None) and (\"probe\" in out):\n",
        "            bce = F.binary_cross_entropy_with_logits(out[\"probe\"], y.float())\n",
        "            loss = loss + self.cfg.probe_weight * bce\n",
        "            logs[\"probe_bce\"] = float(bce.item())\n",
        "        return loss, logs\n",
        "\n",
        "def train_vae(cfg: VAEConfig, train_set: NRPSDataset, test_set: Optional[NRPSDataset] = None,\n",
        "              y_train: Optional[torch.Tensor] = None, y_test: Optional[torch.Tensor] = None,\n",
        "              ckpt_path: Optional[str] = None, use_early_stopping: bool = True) -> Tuple[MultiInputVAE, Dict[str, Any]]:\n",
        "    model = MultiInputVAE(cfg).to(cfg.device)\n",
        "    opt = torch.optim.AdamW(model.parameters(), lr=cfg.lr, weight_decay=cfg.weight_decay)\n",
        "    crit = ELBOLoss(cfg)\n",
        "\n",
        "    dl_train = DataLoader(train_set, batch_size=cfg.batch_size, shuffle=True, drop_last=False)\n",
        "    dl_test = DataLoader(test_set, batch_size=cfg.batch_size, shuffle=False) if test_set is not None else None\n",
        "\n",
        "    history = {\"train\": [], \"test\": []}\n",
        "    step = 0\n",
        "    train_losses_per_epoch, test_losses_per_epoch = [], []\n",
        "    train_recon_per_epoch, test_recon_per_epoch = [], []\n",
        "    train_kl_per_epoch, test_kl_per_epoch = [], []\n",
        "\n",
        "    best_test = math.inf\n",
        "    patience, trigger_times = 5, 0\n",
        "\n",
        "    for epoch in range(cfg.num_epochs):\n",
        "        model.train()\n",
        "        train_losses, train_recons, train_kls = [], [], []\n",
        "        for i, (xb, mb) in enumerate(dl_train):\n",
        "            xb, mb = xb.to(cfg.device), mb.to(cfg.device)\n",
        "            yb = None\n",
        "            if y_train is not None:\n",
        "                yb = y_train[i * cfg.batch_size: i * cfg.batch_size + xb.size(0)].to(cfg.device)\n",
        "\n",
        "            opt.zero_grad(set_to_none=True)\n",
        "            out = model(xb, mb)\n",
        "            loss, logs = crit(xb, out, mb, step=step, y=yb)\n",
        "            loss.backward()\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "            opt.step()\n",
        "\n",
        "            history[\"train\"].append({\"epoch\": epoch, \"step\": step, **logs})\n",
        "            train_losses.append(logs[\"loss\"])\n",
        "            train_recons.append(logs[\"recon\"])\n",
        "            train_kls.append(logs[\"kl\"])\n",
        "            step += 1\n",
        "\n",
        "        train_losses_per_epoch.append(float(np.mean(train_losses)))\n",
        "        train_recon_per_epoch.append(float(np.mean(train_recons)))\n",
        "        train_kl_per_epoch.append(float(np.mean(train_kls)))\n",
        "        print(f\"Epoch {epoch+1}/{cfg.num_epochs}, Train Loss: {train_losses_per_epoch[-1]:.4f}\")\n",
        "\n",
        "        if dl_test is not None:\n",
        "            model.eval()\n",
        "            test_losses, test_recons, test_kls = [], [], []\n",
        "            with torch.no_grad():\n",
        "                for j, (xb, mb) in enumerate(dl_test):\n",
        "                    xb, mb = xb.to(cfg.device), mb.to(cfg.device)\n",
        "                    yb = None\n",
        "                    if y_test is not None:\n",
        "                        yb = y_test[j * cfg.batch_size: j * cfg.batch_size + xb.size(0)].to(cfg.device)\n",
        "                    out = model(xb, mb)\n",
        "                    loss, logs = crit(xb, out, mb, step=step, y=yb)\n",
        "                    test_losses.append(logs[\"loss\"])\n",
        "                    test_recons.append(logs[\"recon\"])\n",
        "                    test_kls.append(logs[\"kl\"])\n",
        "                    history[\"test\"].append({\"epoch\": epoch, \"step\": step, **logs})\n",
        "\n",
        "            test_mean = float(np.mean(test_losses)) if test_losses else math.inf\n",
        "            test_losses_per_epoch.append(test_mean)\n",
        "            test_recon_per_epoch.append(float(np.mean(test_recons)))\n",
        "            test_kl_per_epoch.append(float(np.mean(test_kls)))\n",
        "            print(f\"Epoch {epoch+1}/{cfg.num_epochs}, Test Loss: {test_mean:.4f}\")\n",
        "\n",
        "            # early stopping\n",
        "            if use_early_stopping:\n",
        "                if test_mean < best_test:\n",
        "                    best_test = test_mean\n",
        "                    trigger_times = 0\n",
        "                    if ckpt_path:\n",
        "                        os.makedirs(os.path.dirname(ckpt_path), exist_ok=True)\n",
        "                        torch.save({\"model\": model.state_dict(), \"cfg\": cfg.__dict__}, ckpt_path)\n",
        "                else:\n",
        "                    trigger_times += 1\n",
        "                    if trigger_times >= patience:\n",
        "                        print(f\"Early stopping triggered at epoch {epoch+1}\")\n",
        "                        break\n",
        "    epochs = range(1, len(train_losses_per_epoch) + 1)\n",
        "\n",
        "    plt.figure(figsize=(12,5))\n",
        "    plt.subplot(1,3,1)\n",
        "    plt.plot(epochs, train_losses_per_epoch, label=\"Train\")\n",
        "    if test_losses_per_epoch: plt.plot(epochs, test_losses_per_epoch, label=\"Test\")\n",
        "    plt.title(\"Total Loss\"); plt.xlabel(\"Epoch\"); plt.ylabel(\"Loss\"); plt.legend()\n",
        "\n",
        "    plt.subplot(1,3,2)\n",
        "    plt.plot(epochs, train_recon_per_epoch, label=\"Train\")\n",
        "    if test_recon_per_epoch: plt.plot(epochs, test_recon_per_epoch, label=\"Test\")\n",
        "    plt.title(\"Reconstruction Loss\"); plt.xlabel(\"Epoch\"); plt.ylabel(\"MSE\"); plt.legend()\n",
        "\n",
        "    plt.subplot(1,3,3)\n",
        "    plt.plot(epochs, train_kl_per_epoch, label=\"Train\")\n",
        "    if test_kl_per_epoch: plt.plot(epochs, test_kl_per_epoch, label=\"Test\")\n",
        "    plt.title(\"KL Divergence\"); plt.xlabel(\"Epoch\"); plt.ylabel(\"KL\"); plt.legend()\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(\"diagnostics.png\")\n",
        "    plt.show()\n",
        "    plt.close()\n",
        "    xb, mb = next(iter(DataLoader(test_set, batch_size=5)))  # small batch\n",
        "    xb, mb = xb.to(cfg.device), mb.to(cfg.device)\n",
        "    with torch.no_grad():\n",
        "        out = model(xb, mb)\n",
        "    x_hat = out[\"x_hat\"].cpu()\n",
        "    xb_cpu = xb.cpu()\n",
        "\n",
        "    for i in range(min(5, xb_cpu.size(0))):\n",
        "        plt.figure(figsize=(10,4))\n",
        "        plt.plot(xb_cpu[i].flatten(), label=\"Original\", alpha=0.7)\n",
        "        plt.plot(x_hat[i].flatten(), label=\"Reconstruction\", alpha=0.7)\n",
        "        plt.title(f\"Sample {i}: Original vs Reconstruction\")\n",
        "        plt.legend()\n",
        "        plt.show()\n",
        "    z = out[\"z\"].cpu().numpy()\n",
        "    plt.figure(figsize=(6,6))\n",
        "    plt.scatter(z[:,0], z[:,1], alpha=0.7)\n",
        "    plt.title(\"Latent space (first 2 dims)\")\n",
        "    plt.xlabel(\"z[0]\"); plt.ylabel(\"z[1]\")\n",
        "    plt.show()\n",
        "\n",
        "    return model, history\n",
        "\n",
        "\n",
        "\n",
        "def split_train_test(features: np.ndarray, test_frac: float = 0.1, seed: int = 42,\n",
        "                     mask: Optional[np.ndarray] = None):\n",
        "    rng = np.random.default_rng(seed)\n",
        "    N = features.shape[0]\n",
        "    idx = np.arange(N)\n",
        "    rng.shuffle(idx)\n",
        "    n_test = int(N * test_frac)\n",
        "    test_idx = idx[:n_test]\n",
        "    train_idx = idx[n_test:]\n",
        "    x_train = features[train_idx]\n",
        "    x_test = features[test_idx]\n",
        "    m_train = mask[train_idx] if mask is not None else None\n",
        "    m_test = mask[test_idx] if mask is not None else None\n",
        "    return (x_train, m_train), (x_test, m_test)\n",
        "\n"
      ],
      "metadata": {
        "id": "Az7TpUwf9hzx"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Training"
      ],
      "metadata": {
        "id": "3RqThK5j9vBH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "    N, R, D = 128, 5, 512\n",
        "    X = data[\"features\"]\n",
        "    mask = (np.abs(X).sum(axis=2) > 1e-8).astype(np.float32)\n",
        "\n",
        "    (xtr, mtr), (xte, mte) = split_train_test(X, test_frac=0.1, seed=17, mask=mask)\n",
        "    ds_tr = NRPSDataset(xtr, mtr)\n",
        "    ds_te = NRPSDataset(xte, mte)\n",
        "    cfg = VAEConfig(\n",
        "    d_in=1295,\n",
        "    n_regions=5,\n",
        "    d_model=256,\n",
        "    n_heads=8,\n",
        "    n_layers=2,\n",
        "    d_ff=512,\n",
        "    latent_dim=128,\n",
        "    beta=2,\n",
        "    kl_warmup_steps=500,\n",
        "    dropout=0.2,\n",
        "    decoder_hidden=256,\n",
        "    per_region_decoders=True,\n",
        "    lr=1e-4,\n",
        "    weight_decay=1e-3,\n",
        "    batch_size=64,\n",
        "    num_epochs=100,\n",
        "    use_probe=True,\n",
        "    probe_weight=0.1\n",
        ")\n",
        "\n",
        "    model, hist = train_vae(cfg, ds_tr, ds_te, ckpt_path=\"./checkpoints/mi_vae.pt\", use_early_stopping=False)\n",
        "    print(\"Done. Last train loss:\", hist[\"train\"][-1][\"loss\"])\n",
        "    print(\"Loss plot saved as 'loss_plot.png'\")"
      ],
      "metadata": {
        "id": "LIf_uXLU9xjQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Evaluation"
      ],
      "metadata": {
        "id": "zKYHLt6W9829"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cfg = VAEConfig(\n",
        "    d_in=1295,\n",
        "    n_regions=5,\n",
        "    d_model=256,\n",
        "    n_heads=8,\n",
        "    n_layers=2,\n",
        "    d_ff=512,\n",
        "    latent_dim=128,\n",
        "    beta=2,\n",
        "    kl_warmup_steps=500,\n",
        "    dropout=0.2,\n",
        "    decoder_hidden=256,\n",
        "    per_region_decoders=True,\n",
        "    lr=1e-4,\n",
        "    weight_decay=1e-3,\n",
        "    batch_size=64,\n",
        "    num_epochs=100,\n",
        "    use_probe=True,\n",
        "    probe_weight=0.1\n",
        ")\n",
        "model = MultiInputVAE(cfg).to(cfg.device)"
      ],
      "metadata": {
        "id": "5IewfIYL9-zT"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.load_state_dict(torch.load(\"/content/MI-vae-v1_checkpoint.pt\"))\n",
        "model.eval()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z77hstCp-AjW",
        "outputId": "a5ebb484-5513-4373-e8cc-51d1d8a01961"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "MultiInputVAE(\n",
              "  (encoder): MultiRegionEncoder(\n",
              "    (input_proj): Linear(in_features=1295, out_features=256, bias=True)\n",
              "    (encoder): TransformerEncoder(\n",
              "      (layers): ModuleList(\n",
              "        (0-1): 2 x TransformerEncoderLayer(\n",
              "          (self_attn): MultiheadAttention(\n",
              "            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
              "          )\n",
              "          (linear1): Linear(in_features=256, out_features=512, bias=True)\n",
              "          (dropout): Dropout(p=0.2, inplace=False)\n",
              "          (linear2): Linear(in_features=512, out_features=256, bias=True)\n",
              "          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
              "          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
              "          (dropout1): Dropout(p=0.2, inplace=False)\n",
              "          (dropout2): Dropout(p=0.2, inplace=False)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (segpos): SegmentPositionalEmbedding(\n",
              "      (proj): Linear(in_features=1, out_features=256, bias=True)\n",
              "      (segment): Embedding(5, 256)\n",
              "      (pos): Embedding(5, 256)\n",
              "    )\n",
              "    (layernorm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
              "    (to_mu): Linear(in_features=256, out_features=128, bias=True)\n",
              "    (to_logvar): Linear(in_features=256, out_features=128, bias=True)\n",
              "  )\n",
              "  (decoder): MultiRegionDecoder(\n",
              "    (from_z): Sequential(\n",
              "      (0): Linear(in_features=128, out_features=256, bias=True)\n",
              "      (1): GELU(approximate='none')\n",
              "      (2): Linear(in_features=256, out_features=1280, bias=True)\n",
              "      (3): GELU(approximate='none')\n",
              "    )\n",
              "    (decoders): ModuleList(\n",
              "      (0-4): 5 x Sequential(\n",
              "        (0): Linear(in_features=256, out_features=256, bias=True)\n",
              "        (1): GELU(approximate='none')\n",
              "        (2): Linear(in_features=256, out_features=1295, bias=True)\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (probe): Sequential(\n",
              "    (0): Linear(in_features=128, out_features=64, bias=True)\n",
              "    (1): GELU(approximate='none')\n",
              "    (2): Linear(in_features=64, out_features=1, bias=True)\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Z9Ug_Oxs_wEV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Anomally detection"
      ],
      "metadata": {
        "id": "S0N78FHsSN_9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from sklearn.mixture import GaussianMixture\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from torch.utils.data import DataLoader\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Assume your model classes (VAEConfig, MultiInputVAE, etc.) are defined above this code\n",
        "\n",
        "class AnomalyScorer:\n",
        "    \"\"\"\n",
        "    Designs and computes an anomaly score for the MultiInputVAE.\n",
        "\n",
        "    This scorer combines two main components:\n",
        "    1. Weighted Reconstruction Error: Tailored for the multi-input nature of the model,\n",
        "       this score weights errors in each region based on how hard that region is to\n",
        "       reconstruct on average. It amplifies significant deviations in normally stable regions.\n",
        "    2. Latent Space Density: It models the distribution of 'normal' data in the latent\n",
        "       space using a Gaussian Mixture Model. Samples that fall into low-density areas\n",
        "       are considered anomalous.\n",
        "\n",
        "    The final score is a weighted combination of these two normalized components.\n",
        "    \"\"\"\n",
        "    def __init__(self, model: MultiInputVAE, cfg: VAEConfig):\n",
        "        self.model = model\n",
        "        self.cfg = cfg\n",
        "        self.model.eval()\n",
        "        self.device = cfg.device\n",
        "\n",
        "        # These will be fitted on the normal training data\n",
        "        self.beta_weights = None\n",
        "        self.gmm = None\n",
        "        self.scaler_recon = StandardScaler()\n",
        "        self.scaler_latent = StandardScaler()\n",
        "\n",
        "    def fit(self, train_loader: DataLoader):\n",
        "        \"\"\"\n",
        "        Fit the scorer on normal data. This calculates the beta weights for the\n",
        "        reconstruction score and fits the GMM for the latent score.\n",
        "        \"\"\"\n",
        "        print(\"Fitting Anomaly Scorer on normal training data...\")\n",
        "\n",
        "        all_z = []\n",
        "        # A list of lists, where each inner list holds errors for a region\n",
        "        per_region_errors = [[] for _ in range(self.cfg.n_regions)]\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for xb, mb in tqdm(train_loader, desc=\"Calculating stats\"):\n",
        "                xb, mb = xb.to(self.device), mb.to(self.device)\n",
        "\n",
        "                out = self.model(xb, mb)\n",
        "                x_hat, z = out[\"x_hat\"], out[\"z\"]\n",
        "\n",
        "                # Store latent vectors\n",
        "                all_z.append(z.cpu().numpy())\n",
        "\n",
        "                # Calculate and store per-region reconstruction errors (MSE)\n",
        "                # Shape: (B, R, D) -> (B, R)\n",
        "                errors = F.mse_loss(x_hat, xb, reduction='none').mean(dim=-1)\n",
        "\n",
        "                # Apply mask: we only consider errors for present regions\n",
        "                # The mask `mb` has shape (B, R)\n",
        "                masked_errors = errors * mb\n",
        "\n",
        "                # Iterate over batch and regions to collect valid errors\n",
        "                for b in range(xb.size(0)):\n",
        "                    for r in range(self.cfg.n_regions):\n",
        "                        if mb[b, r] > 0: # If the region is present\n",
        "                            per_region_errors[r].append(masked_errors[b, r].item())\n",
        "\n",
        "        # --- 1. Compute Beta Weights for Reconstruction Score ---\n",
        "        # Calculate the average error for each region\n",
        "        avg_region_errors = np.array([np.mean(errs) if errs else 1.0 for errs in per_region_errors])\n",
        "\n",
        "        # Calculate the global average error\n",
        "        global_avg_error = np.mean(avg_region_errors)\n",
        "\n",
        "        # Calculate beta weights (inverse proportionality)\n",
        "        # Add a small epsilon to avoid division by zero\n",
        "        self.beta_weights = global_avg_error / (avg_region_errors + 1e-8)\n",
        "        self.beta_weights = torch.from_numpy(self.beta_weights).float().to(self.device)\n",
        "        print(f\"Computed Beta Weights per region: {self.beta_weights.cpu().numpy()}\")\n",
        "\n",
        "        # --- 2. Fit GMM on Latent Space ---\n",
        "        all_z_np = np.concatenate(all_z, axis=0)\n",
        "        # Using K=5 as suggested in the paper, but this can be tuned\n",
        "        print(f\"Fitting GMM (K=5) on {all_z_np.shape[0]} latent vectors...\")\n",
        "        self.gmm = GaussianMixture(n_components=5, random_state=42, covariance_type='diag')\n",
        "        self.gmm.fit(all_z_np)\n",
        "\n",
        "        # --- 3. Fit Scalers for Normalization ---\n",
        "        # We need to calculate the raw scores for the training data to fit the scalers\n",
        "        print(\"Calculating raw scores for normalization...\")\n",
        "        raw_recon_scores = []\n",
        "        raw_latent_scores = []\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for xb, mb in tqdm(train_loader, desc=\"Normalizing scores\"):\n",
        "                xb, mb = xb.to(self.device), mb.to(self.device)\n",
        "                recon_s, latent_s = self._get_raw_scores(xb, mb)\n",
        "                raw_recon_scores.append(recon_s.cpu().numpy())\n",
        "                raw_latent_scores.append(latent_s) # Already a numpy array\n",
        "\n",
        "        self.scaler_recon.fit(np.concatenate(raw_recon_scores).reshape(-1, 1))\n",
        "        self.scaler_latent.fit(np.concatenate(raw_latent_scores).reshape(-1, 1))\n",
        "        print(\"Anomaly Scorer is ready.\")\n",
        "\n",
        "    def _get_raw_scores(self, xb: torch.Tensor, mb: torch.Tensor) -> Tuple[torch.Tensor, np.ndarray]:\n",
        "        \"\"\"Helper to compute the two un-normalized score components.\"\"\"\n",
        "        out = self.model(xb, mb)\n",
        "        x_hat, z = out[\"x_hat\"], out[\"z\"]\n",
        "\n",
        "        # Raw weighted reconstruction score\n",
        "        per_region_mse = F.mse_loss(x_hat, xb, reduction='none').mean(dim=-1) # (B, R)\n",
        "        weighted_errors = per_region_mse * self.beta_weights.unsqueeze(0)\n",
        "\n",
        "        # Apply mask and sum over regions\n",
        "        score_recon = (weighted_errors * mb).sum(dim=1) / (mb.sum(dim=1) + 1e-8)\n",
        "\n",
        "        # Raw latent density score (log-likelihood)\n",
        "        score_latent = self.gmm.score_samples(z.cpu().numpy())\n",
        "\n",
        "        return score_recon, score_latent\n",
        "\n",
        "    def score(self, data_loader: DataLoader, w_recon: float = 1.0, w_latent: float = 1.0) -> np.ndarray:\n",
        "        \"\"\"\n",
        "        Calculates the final anomaly score for a given dataset.\n",
        "\n",
        "        Args:\n",
        "            data_loader: DataLoader for the dataset to be scored.\n",
        "            w_recon: Weight for the reconstruction score component.\n",
        "            w_latent: Weight for the latent density score component.\n",
        "\n",
        "        Returns:\n",
        "            A numpy array of final anomaly scores for each sample.\n",
        "        \"\"\"\n",
        "        if self.gmm is None or self.beta_weights is None:\n",
        "            raise RuntimeError(\"Scorer has not been fitted. Call .fit(train_loader) first.\")\n",
        "\n",
        "        self.model.eval()\n",
        "        final_scores = []\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for xb, mb in tqdm(data_loader, desc=\"Scoring samples\"):\n",
        "                xb, mb = xb.to(self.device), mb.to(self.device)\n",
        "\n",
        "                score_recon_raw, score_latent_raw = self._get_raw_scores(xb, mb)\n",
        "\n",
        "                # Normalize scores\n",
        "                norm_recon = self.scaler_recon.transform(score_recon_raw.cpu().numpy().reshape(-1, 1))\n",
        "                norm_latent = self.scaler_latent.transform(score_latent_raw.reshape(-1, 1))\n",
        "\n",
        "                # Combine scores: higher is more anomalous\n",
        "                # Note the minus sign for latent score, as low log-likelihood is anomalous\n",
        "                combined_score = (w_recon * norm_recon) - (w_latent * norm_latent)\n",
        "                final_scores.append(combined_score.flatten())\n",
        "\n",
        "        return np.concatenate(final_scores)"
      ],
      "metadata": {
        "id": "b9uRdvqfSQiL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.isotonic import IsotonicRegression\n",
        "\n",
        "# --- USAGE EXAMPLE ---\n",
        "\n",
        "# 1. First, train your VAE model as you did before\n",
        "# model, hist = train_vae(cfg, ds_tr, ds_te, ...)\n",
        "\n",
        "# 2. Create the scorer and fit it on the NORMAL training data\n",
        "# NOTE: Use a DataLoader with the same batch size as training for consistency\n",
        "dl_tr_for_scorer = DataLoader(ds_tr, batch_size=cfg.batch_size, shuffle=False)\n",
        "scorer = AnomalyScorer(model, cfg)\n",
        "scorer.fit(dl_tr_for_scorer)\n",
        "\n",
        "# 3. Score your test set (or any new data)\n",
        "dl_te = DataLoader(ds_te, batch_size=cfg.batch_size, shuffle=False)\n",
        "raw_anomaly_scores = scorer.score(dl_te)\n",
        "\n",
        "# Assume you have labels for your test set to see how well it works\n",
        "# y_test = ... (0 for normal, 1 for anomaly)\n",
        "# from sklearn.metrics import roc_auc_score\n",
        "# print(f\"AUC Score: {roc_auc_score(y_test, raw_anomaly_scores)}\")\n",
        "\n",
        "\n",
        "# --- CALIBRATION ---\n",
        "\n",
        "# 4. Assume you have a calibration set (X_calib, y_calib)\n",
        "# You would create a DataLoader for it: ds_calib = NRPSDataset(X_calib, m_calib)\n",
        "# dl_calib = DataLoader(ds_calib, batch_size=cfg.batch_size)\n",
        "\n",
        "# Get the raw scores for the calibration set\n",
        "# calib_scores = scorer.score(dl_calib).reshape(-1, 1)\n",
        "\n",
        "# Fit a calibration model (Platt Scaling)\n",
        "# platt_scaler = LogisticRegression()\n",
        "# platt_scaler.fit(calib_scores, y_calib)\n",
        "\n",
        "# Now you can get probabilities for your test scores\n",
        "# test_scores_reshaped = raw_anomaly_scores.reshape(-1, 1)\n",
        "# probabilities = platt_scaler.predict_proba(test_scores_reshaped)[:, 1]\n",
        "\n",
        "# --- DECISION BAND ---\n",
        "# As recommended, use a decision band instead of a hard threshold\n",
        "def classify_sample(probability: float):\n",
        "    if probability > 0.75: # High threshold\n",
        "        return \"Likely Anomaly\"\n",
        "    elif probability < 0.25: # Low threshold\n",
        "        return \"Likely Normal\"\n",
        "    else:\n",
        "        return \"Uncertain (Requires Review)\"\n",
        "\n",
        "for p in probabilities[:10]:\n",
        "   print(f\"Probability: {p:.3f} -> Classification: {classify_sample(p)}\")"
      ],
      "metadata": {
        "id": "loctdrNkSZHg"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}